{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import boto3\n",
    "\n",
    "import zipfile\n",
    "import py7zr\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ROOT = Path(\"C:/Users/hp/Desktop/data-pipeline/data/urban-feature-extraction\")\n",
    "ROOT = Path.cwd().parent / \"data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetETL:\n",
    "    def __init__(self, root:Path, urls:dict, low_storage_mode:bool):\n",
    "        download_dir = (root / \"downloads\") \n",
    "        if not root.exists():\n",
    "            print(\"Root directory DNE, creating new directory\")\n",
    "            root.mkdir()\n",
    "            download_dir.mkdir()\n",
    "        self.root_dir = root\n",
    "        self.low_storage_mode = low_storage_mode\n",
    "\n",
    "        if not download_dir.exists():\n",
    "            print(\"Downloads directory DNE, creating new directory\")\n",
    "            download_dir.mkdir()\n",
    "        self.download_dir = download_dir \n",
    "        #Source URLs is Dict[file_name:str, url:str]\n",
    "        self.source_urls:dict = urls\n",
    "\n",
    "    #TODO Raise error if these are not implemented in child function\n",
    "    def download(self):\n",
    "        pass\n",
    "    def extract(self):\n",
    "        pass\n",
    "    def catalog(self):\n",
    "        pass\n",
    "\n",
    "    def Extract():\n",
    "        #Download and Stage Dataset on Disk.\n",
    "        self.download()\n",
    "        self.extract()\n",
    "        self.catalog() \n",
    "\n",
    "    def Transform():\n",
    "        pass\n",
    "    def Load():\n",
    "        pass\n",
    "\n",
    "    def clear_downloads_directory(self):\n",
    "        downloaded_files:list = [path for path in self.download_dir.iterdir()]\n",
    "        self.delete_files(downloaded_files)\n",
    "\n",
    "    #Internal Methods\n",
    "\n",
    "    def _get_source_urls(self, urls_list:list) -> dict:\n",
    "        #List[url:str] -> Dict[file_name:str, url:str]\n",
    "\n",
    "        return {Path(url).name : url for url in urls_list}\n",
    "\n",
    "    def _download_file(self, url:str, file_path:Path, chunk_size:int = 1024*1024):\n",
    "        #Download from url and save to disk at file_path\n",
    "\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            with open(file_path, \"wb\") as f, tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk: \n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "\n",
    "    def _download_source_urls(self, download_dir:Path = Path(\"\")):\n",
    "        if download_dir == Path(\"\"):\n",
    "            download_dir = self.download_dir\n",
    "        #Download files from self.source_urls, skip if already_downloaded\n",
    "\n",
    "        for file_name, url in self.source_urls.items():\n",
    "            file_path = download_dir / file_name \n",
    "\n",
    "            if file_path.exists():\n",
    "                #TODO: Check for downloaded file size as well\n",
    "                print(\"File Already Exists, Skipping\")\n",
    "                continue\n",
    "\n",
    "            self._download_file(url, file_path)\n",
    "\n",
    "    def _extract_zip(self, zip_file_path:Path, target_dir:Path, dirs_to_be_extracted = list()):\n",
    "        #Extract specified dirs from zip archive, extract all dirs if not specified\n",
    "\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip:\n",
    "            #If dirs_to_be_extracted is an empty list, extract entire archive and exit\n",
    "            if not extract_dirs:\n",
    "                zip.extractall(target_dir); return\n",
    "            #Otherwise, extract all files under specified dirs\n",
    "            #For each file in archive, extract if it's under any specified dir\n",
    "            for member in zip.infolist():\n",
    "                for foldername in dirs_to_be_extracted:\n",
    "                    if foldername in member.filename:\n",
    "                        #TODO: Add tqdm progress bar for extraction\n",
    "                        zip.extract(member, target_dir)\n",
    "\n",
    "    def _extract_7zip(self, zip_file_path:Path, target_dir:Path, dirs_to_be_extracted = list()):\n",
    "        #Extract specified dirs from 7zip archive, extract all dirs if not specified\n",
    "\n",
    "        with py7zr.SevenZipFile(zip_file_path, 'r') as zip:\n",
    "\n",
    "        #If dirs_to_be_extracted is an empty list, extract entire archive\n",
    "            if not dirs_to_be_extracted: \n",
    "                zip.extractall(target_dir); return\n",
    "\n",
    "        #Otherwise, extract all files under specified dirs\n",
    "        #For each file in archive, extract if it's under any specified dir\n",
    "            for member in zip.getnames():\n",
    "                for foldername in dirs_to_be_extracted:\n",
    "                    if foldername in member:\n",
    "                        zip.extract(target_dir, member)\n",
    "                        zip.reset()\n",
    "\n",
    "    def _merge_multivolume_archive(self, multivolume_paths:list, target_zip_path:Path):\n",
    "        #Combine multivolume archive files into a single archive\n",
    "\n",
    "        with open(target_zip_path, 'ab') as outfile:\n",
    "            for volume_path in multivolume_paths:\n",
    "                with open(volume_path, 'rb') as infile:\n",
    "                    outfile.write(infile.read())\n",
    "\n",
    "\n",
    "    def _validate_files(self, file_paths:list, validation_file_names:list, dir:Path):\n",
    "        source_file_names = set(validation_file_names)\n",
    "        file_names = set([path.name for path in file_paths]) \n",
    "        return source_file_names == file_names\n",
    "\n",
    "    def _delete_files(self, file_paths:list):\n",
    "        #Delete list of files if they exist, print warnings if they dont. \n",
    "\n",
    "        for file_path in file_paths:\n",
    "            if file_path.exists():\n",
    "                file_path.unlink()\n",
    "            else:\n",
    "                print(f\"Error Deleting {file_path.name}\")\n",
    "\n",
    "    def _get_raster_metadata(self, raster_path:Path):\n",
    "        #Return Shape, Reference Frame and Transformation Matrix of a Raster File\n",
    "\n",
    "        with rio.open(raster_path) as raster:\n",
    "            return raster.shape, str(raster.crs), tuple(raster.transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InriaETL(DatasetETL):\n",
    "    def __init__(self, root:Path, low_storage_mode:bool = True):\n",
    "        super().__init__(\n",
    "            root = root, \n",
    "            urls = self._get_source_urls([\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.001\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.002\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.003\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.004\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.005\"\n",
    "                                        ]),\n",
    "            low_storage_mode = low_storage_mode\n",
    "        )\n",
    "\n",
    "        self.locations = [\"austin\", \"chicago\", \"kitsap\", \"tyrol-w\", \"vienna\"]\n",
    "        self.files_list = [f\"{location}{num}.tif\" for location in self.locations for num in range(1, 37)]\n",
    "\n",
    "        dataset_dir = self.root_dir / \"AerialImageDataset\" / \"train\"\n",
    "        self.image_dir = dataset_dir / \"images\"\n",
    "        self.mask_dir = dataset_dir / \"gt\"\n",
    "\n",
    "    def download(self):\n",
    "        self._download_source_urls(self.download_dir)\n",
    "    \n",
    "    def extract(self):\n",
    "        #Verify Download\n",
    "        downloaded_7zip_files_list:list = [path for path in self.download_dir.glob(\"*.7z.*\")]\n",
    "\n",
    "        #TODO: Implement _validate_files(files_paths_list:list)\n",
    "\n",
    "        if not self._validate_files(downloaded_7zip_files_list):\n",
    "            print(\"Missing volumes\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Found all volumes\")\n",
    "\n",
    "        #Merge Volumes To One Archive\n",
    "        merged_7zip_path = self.download_dir / \"aerialimagelabeling-merged.7z\" \n",
    "        if not merged_7zip_path.exists():\n",
    "            print(\"Merged archive not found\")\n",
    "            print(f\"Merging volumes to {merged_7zip_path.name}\")\n",
    "            self._merge_multivolume_archive(downloaded_7zip_files_list, merged_7zip_path)\n",
    "        else:\n",
    "            print(\"Merged archive found\")\n",
    "\n",
    "        #Delete downloaded volumes        \n",
    "        if self.low_storage_mode:\n",
    "            print(\"Deleting downloaded volumes\")\n",
    "            self._delete_files(downloaded_7zip_files_list)\n",
    "        \n",
    "        print(\"Decompressing merged archive\")\n",
    "        self._extract_7zip(merged_7zip_path, self.download_dir)\n",
    "        print(\"Decompression complete\")\n",
    "\n",
    "        if self.low_storage_mode:\n",
    "            print(\"Deleting merged archive\")\n",
    "            self._delete_files([merged_7zip_path])\n",
    "\n",
    "        #Extract Train Folder \n",
    "        dataset_zipfile_name = \"NEW2-AerialImageDataset.zip\"\n",
    "        dataset_zipfile_path = self.download_dir / dataset_zipfile_name \n",
    "        if not dataset_zipfile_path.exists():\n",
    "            print(f\"{dataset_zipfile_name} Not Found\")\n",
    "            return\n",
    "        print(\"Extracting Dataset Folder\")\n",
    "        self._extract_zip(dataset_zipfile_path, self.root_dir, [\"train\"])\n",
    "        print(\"Extraction Complete\")\n",
    "\n",
    "        if self.low_storage_mode: \n",
    "            print(\"Deleting dataset archive\")\n",
    "            self._delete_files([dataset_zipfile_path])\n",
    "\n",
    "    #TODO: Implement Verify Extraction\n",
    "    #TODO: Move Catalog To DatasetETL Class ?\n",
    "        \n",
    "    def catalog(self):\n",
    "        self.dataset = pd.DataFrame({\"name\": list(map(lambda x: x.name, self.mask_dir.iterdir()))})\n",
    "\n",
    "        #TODO: Convert These to Dataframe ApplyMaps\n",
    "        metadata = list(map(self._get_raster_metadata, self.mask_dir.iterdir()))\n",
    "        shapes, crses, transforms = zip(*metadata)\n",
    "\n",
    "        data = {\n",
    "            \"name\": extracted_file_names,\n",
    "            \"shape\": shapes,\n",
    "            \"crs\": crses,\n",
    "            \"transform\": transforms,\n",
    "            \"split\": [self._get_split(x) for x in extracted_file_names],\n",
    "            \"mask_path\": [self.mask_dir / x for x in extracted_file_names],\n",
    "            \"image_path\": [self.image_dir / x for x in extracted_file_names],\n",
    "        }\n",
    "        self.dataset = pd.DataFrame(data)\n",
    "    \n",
    "    def _get_split(self, file_name:str):\n",
    "        \"\"\"First 6 (16.67%) in every region for testing \"\"\"\n",
    "        numbers = [char for char in file_name if char.isdigit()]\n",
    "        if int(''.join(numbers)) <= 6:\n",
    "            return \"test\"\n",
    "        return \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria = InriaETL(ROOT / \"inria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "/home/sambhav/dev/urban-feature-extraction/data/inria/AerialImageDataset/train/gt/austin1.tif: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mrasterio/_base.pyx:308\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_base.pyx:219\u001b[0m, in \u001b[0;36mrasterio._base.open_dataset\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_err.pyx:221\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: /home/sambhav/dev/urban-feature-extraction/data/inria/AerialImageDataset/train/gt/austin1.tif: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inria\u001b[39m.\u001b[39;49mcatalog()\n\u001b[1;32m      2\u001b[0m inria\u001b[39m.\u001b[39mdataset\n",
      "Cell \u001b[0;32mIn[15], line 76\u001b[0m, in \u001b[0;36mInriaETL.catalog\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcatalog\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     metadata \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_raster_metadata((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_dir \u001b[39m/\u001b[39m x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles_list]\n\u001b[1;32m     77\u001b[0m     shapes, crses, transforms \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mmetadata)\n\u001b[1;32m     78\u001b[0m     data \u001b[39m=\u001b[39m {\n\u001b[1;32m     79\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles_list,\n\u001b[1;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m: shapes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_dir \u001b[39m/\u001b[39m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles_list],\n\u001b[1;32m     86\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[15], line 76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcatalog\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     metadata \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_raster_metadata((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask_dir \u001b[39m/\u001b[39;49m x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles_list]\n\u001b[1;32m     77\u001b[0m     shapes, crses, transforms \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mmetadata)\n\u001b[1;32m     78\u001b[0m     data \u001b[39m=\u001b[39m {\n\u001b[1;32m     79\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles_list,\n\u001b[1;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m: shapes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_dir \u001b[39m/\u001b[39m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles_list],\n\u001b[1;32m     86\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[11], line 133\u001b[0m, in \u001b[0;36mDatasetETL._get_raster_metadata\u001b[0;34m(self, raster_path)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_raster_metadata\u001b[39m(\u001b[39mself\u001b[39m, raster_path:Path):\n\u001b[1;32m    131\u001b[0m     \u001b[39m#Return Shape, Reference Frame and Transformation Matrix of a Raster File\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mwith\u001b[39;00m rio\u001b[39m.\u001b[39;49mopen(raster_path) \u001b[39mas\u001b[39;00m raster:\n\u001b[1;32m    134\u001b[0m         \u001b[39mreturn\u001b[39;00m raster\u001b[39m.\u001b[39mshape, \u001b[39mstr\u001b[39m(raster\u001b[39m.\u001b[39mcrs), \u001b[39mtuple\u001b[39m(raster\u001b[39m.\u001b[39mtransform)\n",
      "File \u001b[0;32m~/bin/anaconda3/envs/ml/lib/python3.9/site-packages/rasterio/env.py:451\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    448\u001b[0m     session \u001b[39m=\u001b[39m DummySession()\n\u001b[1;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m env_ctor(session\u001b[39m=\u001b[39msession):\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/bin/anaconda3/envs/ml/lib/python3.9/site-packages/rasterio/__init__.py:304\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m path \u001b[39m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     dataset \u001b[39m=\u001b[39m DatasetReader(path, driver\u001b[39m=\u001b[39;49mdriver, sharing\u001b[39m=\u001b[39;49msharing, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    305\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    306\u001b[0m     dataset \u001b[39m=\u001b[39m get_writer_for_path(path, driver\u001b[39m=\u001b[39mdriver)(\n\u001b[1;32m    307\u001b[0m         path, mode, driver\u001b[39m=\u001b[39mdriver, sharing\u001b[39m=\u001b[39msharing, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    308\u001b[0m     )\n",
      "File \u001b[0;32mrasterio/_base.pyx:310\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: /home/sambhav/dev/urban-feature-extraction/data/inria/AerialImageDataset/train/gt/austin1.tif: No such file or directory"
     ]
    }
   ],
   "source": [
    "inria.catalog()\n",
    "inria.dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massachussets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassachussetsETL(DatasetETL):\n",
    "    mass_urls = {\n",
    "        \"train\": (\"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/train/sat/index.html\", \"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/train/map/index.html\"),\n",
    "        \"test\" : (\"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/test/sat/index.html\", \"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/test/map/index.html\"),\n",
    "        \"val\": (\"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/valid/sat/index.html\", \"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/valid/map/index.html\"),\n",
    "    }\n",
    "    def __init__(self, root:Path):\n",
    "        super().__init__(root, self._get_source_urls())\n",
    "        self.image_dir = self.root_dir / \"images\"\n",
    "        self.image_dir.mkdir(exist_ok=True)\n",
    "        self.mask_dir = self.root_dir / \"masks\"\n",
    "        self.mask_dir.mkdir(exist_ok=True)\n",
    "        self.files_list = [url.split(\"/\")[-1] for url in self.source_urls[0]]\n",
    "\n",
    "    def _get_file_urls(self, url) -> list:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            pattern = r'<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"'\n",
    "            matches = re.findall(pattern, response.text)\n",
    "            return matches\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code)\n",
    "            return []\n",
    "\n",
    "    def _get_source_urls(self) -> tuple:\n",
    "        images = sum([self._get_file_urls(self.mass_urls[key][0]) for key in self.mass_urls.keys()], [])\n",
    "        masks = sum([self._get_file_urls(self.mass_urls[key][1]) for key in self.mass_urls.keys()], [])\n",
    "        return images, masks\n",
    "\n",
    "    def download(self, urls:list, target_dir_path:Path):\n",
    "        for url in urls:\n",
    "            downloaded_file_path = target_dir_path / url.split(\"/\")[-1]\n",
    "            if downloaded_file_path.exists():\n",
    "                print(f\"{url.split('/')[-1]} Exists, Skipping\")\n",
    "                continue\n",
    "            self.download_file(url, downloaded_file_path)\n",
    "    \n",
    "    def download_images(self) -> None:\n",
    "        self.download(self.source_urls[0], self.image_dir)\n",
    "        \n",
    "    def download_masks(self) -> None:\n",
    "        self.download(self.source_urls[1], self.image_dir)\n",
    "        \n",
    "    def _validate_download_dir(self, target_dir_path:Path) -> list:\n",
    "        files_not_downloaded = list()\n",
    "        for file_name in self.files_list:\n",
    "            downloaded_file_path = target_dir_path / file_name\n",
    "            if not downloaded_file_path.exists():\n",
    "                files_not_downloaded.append(downloaded_file_path)\n",
    "        return files_not_downloaded\n",
    "    \n",
    "    def validate_images(self) -> None:\n",
    "        return self._validate_download_dir(self.image_dir)\n",
    "\n",
    "    def validate_masks(self) -> None:\n",
    "        return self._validate_download_dir(self.mask_dir)\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        metadata = [self.get_raster_metadata((self.mask_dir / x)) for x in self.files_list] \n",
    "        shapes, crses, transforms = zip(*metadata)\n",
    "        data = {\n",
    "            \"name\": self.files_list,\n",
    "            \"shape\": shapes, \n",
    "            \"crs\": crses,\n",
    "            \"transform\": transforms,\n",
    "            \"image_path\": [(self.image_dir / x) for x in self.files_list],\n",
    "            \"mask_path\": [(self.mask_dir / x) for x in self.files_list]\n",
    "        }\n",
    "        return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "massachussets = MassachussetsETL(ROOT / \"massachussets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22678915_15.tiff Exists, Skipping\n",
      "22678930_15.tiff Exists, Skipping\n",
      "22678945_15.tiff Exists, Skipping\n",
      "22678960_15.tiff Exists, Skipping\n",
      "22678975_15.tiff Exists, Skipping\n",
      "22678990_15.tiff Exists, Skipping\n",
      "22679005_15.tiff Exists, Skipping\n",
      "22679020_15.tiff Exists, Skipping\n",
      "22679035_15.tiff Exists, Skipping\n",
      "22679050_15.tiff Exists, Skipping\n",
      "22828915_15.tiff Exists, Skipping\n",
      "22828945_15.tiff Exists, Skipping\n",
      "22828960_15.tiff Exists, Skipping\n",
      "22828975_15.tiff Exists, Skipping\n",
      "22829005_15.tiff Exists, Skipping\n",
      "22829020_15.tiff Exists, Skipping\n",
      "22829035_15.tiff Exists, Skipping\n",
      "22978870_15.tiff Exists, Skipping\n",
      "22978885_15.tiff Exists, Skipping\n",
      "22978900_15.tiff Exists, Skipping\n",
      "22978915_15.tiff Exists, Skipping\n",
      "22978930_15.tiff Exists, Skipping\n",
      "22978960_15.tiff Exists, Skipping\n",
      "22978975_15.tiff Exists, Skipping\n",
      "22978990_15.tiff Exists, Skipping\n",
      "22979005_15.tiff Exists, Skipping\n",
      "22979020_15.tiff Exists, Skipping\n",
      "22979035_15.tiff Exists, Skipping\n",
      "22979050_15.tiff Exists, Skipping\n",
      "22979065_15.tiff Exists, Skipping\n",
      "23128870_15.tiff Exists, Skipping\n",
      "23128885_15.tiff Exists, Skipping\n",
      "23128900_15.tiff Exists, Skipping\n",
      "23128915_15.tiff Exists, Skipping\n",
      "23128930_15.tiff Exists, Skipping\n",
      "23128945_15.tiff Exists, Skipping\n",
      "23128960_15.tiff Exists, Skipping\n",
      "23128975_15.tiff Exists, Skipping\n",
      "23128990_15.tiff Exists, Skipping\n",
      "23129005_15.tiff Exists, Skipping\n",
      "23129020_15.tiff Exists, Skipping\n",
      "23129035_15.tiff Exists, Skipping\n",
      "23129050_15.tiff Exists, Skipping\n",
      "23129065_15.tiff Exists, Skipping\n",
      "23129125_15.tiff Exists, Skipping\n",
      "23129140_15.tiff Exists, Skipping\n",
      "23129155_15.tiff Exists, Skipping\n",
      "23129170_15.tiff Exists, Skipping\n",
      "23278885_15.tiff Exists, Skipping\n",
      "23278900_15.tiff Exists, Skipping\n",
      "23278915_15.tiff Exists, Skipping\n",
      "23278930_15.tiff Exists, Skipping\n",
      "23278945_15.tiff Exists, Skipping\n",
      "23278960_15.tiff Exists, Skipping\n",
      "23278975_15.tiff Exists, Skipping\n",
      "23278990_15.tiff Exists, Skipping\n",
      "23279005_15.tiff Exists, Skipping\n",
      "23279020_15.tiff Exists, Skipping\n",
      "23279035_15.tiff Exists, Skipping\n",
      "23279050_15.tiff Exists, Skipping\n",
      "23279080_15.tiff Exists, Skipping\n",
      "23279095_15.tiff Exists, Skipping\n",
      "23279140_15.tiff Exists, Skipping\n",
      "23279155_15.tiff Exists, Skipping\n",
      "23279170_15.tiff Exists, Skipping\n",
      "23428900_15.tiff Exists, Skipping\n",
      "23428915_15.tiff Exists, Skipping\n",
      "23428930_15.tiff Exists, Skipping\n",
      "23428945_15.tiff Exists, Skipping\n",
      "23428960_15.tiff Exists, Skipping\n",
      "23428975_15.tiff Exists, Skipping\n",
      "23428990_15.tiff Exists, Skipping\n",
      "23429005_15.tiff Exists, Skipping\n",
      "23429035_15.tiff Exists, Skipping\n",
      "23429050_15.tiff Exists, Skipping\n",
      "23429065_15.tiff Exists, Skipping\n",
      "23429095_15.tiff Exists, Skipping\n",
      "23429125_15.tiff Exists, Skipping\n",
      "23429140_15.tiff Exists, Skipping\n",
      "23429170_15.tiff Exists, Skipping\n",
      "23578915_15.tiff Exists, Skipping\n",
      "23578930_15.tiff Exists, Skipping\n",
      "23578945_15.tiff Exists, Skipping\n",
      "23578975_15.tiff Exists, Skipping\n",
      "23578990_15.tiff Exists, Skipping\n",
      "23579020_15.tiff Exists, Skipping\n",
      "23579035_15.tiff Exists, Skipping\n",
      "23579065_15.tiff Exists, Skipping\n",
      "23579080_15.tiff Exists, Skipping\n",
      "23579095_15.tiff Exists, Skipping\n",
      "23579110_15.tiff Exists, Skipping\n",
      "23579125_15.tiff Exists, Skipping\n",
      "23579140_15.tiff Exists, Skipping\n",
      "23728840_15.tiff Exists, Skipping\n",
      "23728945_15.tiff Exists, Skipping\n",
      "23728960_15.tiff Exists, Skipping\n",
      "23728975_15.tiff Exists, Skipping\n",
      "23728990_15.tiff Exists, Skipping\n",
      "23729005_15.tiff Exists, Skipping\n",
      "23729020_15.tiff Exists, Skipping\n",
      "23729050_15.tiff Exists, Skipping\n",
      "23729065_15.tiff Exists, Skipping\n",
      "23729080_15.tiff Exists, Skipping\n",
      "23729095_15.tiff Exists, Skipping\n",
      "23729110_15.tiff Exists, Skipping\n",
      "23878915_15.tiff Exists, Skipping\n",
      "23878930_15.tiff Exists, Skipping\n",
      "23878945_15.tiff Exists, Skipping\n",
      "23878975_15.tiff Exists, Skipping\n",
      "23878990_15.tiff Exists, Skipping\n",
      "23879020_15.tiff Exists, Skipping\n",
      "23879035_15.tiff Exists, Skipping\n",
      "23879050_15.tiff Exists, Skipping\n",
      "23879065_15.tiff Exists, Skipping\n",
      "23879095_15.tiff Exists, Skipping\n",
      "23879110_15.tiff Exists, Skipping\n",
      "24029035_15.tiff Exists, Skipping\n",
      "24029050_15.tiff Exists, Skipping\n",
      "24029065_15.tiff Exists, Skipping\n",
      "24029080_15.tiff Exists, Skipping\n",
      "24029110_15.tiff Exists, Skipping\n",
      "24179020_15.tiff Exists, Skipping\n",
      "24179035_15.tiff Exists, Skipping\n",
      "24179050_15.tiff Exists, Skipping\n",
      "24179080_15.tiff Exists, Skipping\n",
      "24328840_15.tiff Exists, Skipping\n",
      "24328855_15.tiff Exists, Skipping\n",
      "24328870_15.tiff Exists, Skipping\n",
      "24329020_15.tiff Exists, Skipping\n",
      "24329035_15.tiff Exists, Skipping\n",
      "24329095_15.tiff Exists, Skipping\n",
      "24478840_15.tiff Exists, Skipping\n",
      "24478855_15.tiff Exists, Skipping\n",
      "24478870_15.tiff Exists, Skipping\n",
      "24478885_15.tiff Exists, Skipping\n",
      "24478900_15.tiff Exists, Skipping\n",
      "24479005_15.tiff Exists, Skipping\n",
      "22828930_15.tiff Exists, Skipping\n",
      "22828990_15.tiff Exists, Skipping\n",
      "22829050_15.tiff Exists, Skipping\n",
      "23429020_15.tiff Exists, Skipping\n",
      "23429080_15.tiff Exists, Skipping\n",
      "23578960_15.tiff Exists, Skipping\n",
      "23579005_15.tiff Exists, Skipping\n",
      "23729035_15.tiff Exists, Skipping\n",
      "23879080_15.tiff Exists, Skipping\n",
      "24179065_15.tiff Exists, Skipping\n",
      "22978945_15.tiff Exists, Skipping\n",
      "23429155_15.tiff Exists, Skipping\n",
      "23579050_15.tiff Exists, Skipping\n",
      "23728930_15.tiff Exists, Skipping\n",
      "22678915_15.tif Exists, Skipping\n",
      "22678930_15.tif Exists, Skipping\n",
      "22678945_15.tif Exists, Skipping\n",
      "22678960_15.tif Exists, Skipping\n",
      "22678975_15.tif Exists, Skipping\n",
      "22678990_15.tif Exists, Skipping\n",
      "22679005_15.tif Exists, Skipping\n",
      "22679020_15.tif Exists, Skipping\n",
      "22679035_15.tif Exists, Skipping\n",
      "22679050_15.tif Exists, Skipping\n",
      "22828915_15.tif Exists, Skipping\n",
      "22828945_15.tif Exists, Skipping\n",
      "22828960_15.tif Exists, Skipping\n",
      "22828975_15.tif Exists, Skipping\n",
      "22829005_15.tif Exists, Skipping\n",
      "22829020_15.tif Exists, Skipping\n",
      "22829035_15.tif Exists, Skipping\n",
      "22978870_15.tif Exists, Skipping\n",
      "22978885_15.tif Exists, Skipping\n",
      "22978900_15.tif Exists, Skipping\n",
      "22978915_15.tif Exists, Skipping\n",
      "22978930_15.tif Exists, Skipping\n",
      "22978960_15.tif Exists, Skipping\n",
      "22978975_15.tif Exists, Skipping\n",
      "22978990_15.tif Exists, Skipping\n",
      "22979005_15.tif Exists, Skipping\n",
      "22979020_15.tif Exists, Skipping\n",
      "22979035_15.tif Exists, Skipping\n",
      "22979050_15.tif Exists, Skipping\n",
      "22979065_15.tif Exists, Skipping\n",
      "23128870_15.tif Exists, Skipping\n",
      "23128885_15.tif Exists, Skipping\n",
      "23128900_15.tif Exists, Skipping\n",
      "23128915_15.tif Exists, Skipping\n",
      "23128930_15.tif Exists, Skipping\n",
      "23128945_15.tif Exists, Skipping\n",
      "23128960_15.tif Exists, Skipping\n",
      "23128975_15.tif Exists, Skipping\n",
      "23128990_15.tif Exists, Skipping\n",
      "23129005_15.tif Exists, Skipping\n",
      "23129020_15.tif Exists, Skipping\n",
      "23129035_15.tif Exists, Skipping\n",
      "23129050_15.tif Exists, Skipping\n",
      "23129065_15.tif Exists, Skipping\n",
      "23129125_15.tif Exists, Skipping\n",
      "23129140_15.tif Exists, Skipping\n",
      "23129155_15.tif Exists, Skipping\n",
      "23129170_15.tif Exists, Skipping\n",
      "23278885_15.tif Exists, Skipping\n",
      "23278900_15.tif Exists, Skipping\n",
      "23278915_15.tif Exists, Skipping\n",
      "23278930_15.tif Exists, Skipping\n",
      "23278945_15.tif Exists, Skipping\n",
      "23278960_15.tif Exists, Skipping\n",
      "23278975_15.tif Exists, Skipping\n",
      "23278990_15.tif Exists, Skipping\n",
      "23279005_15.tif Exists, Skipping\n",
      "23279020_15.tif Exists, Skipping\n",
      "23279035_15.tif Exists, Skipping\n",
      "23279050_15.tif Exists, Skipping\n",
      "23279080_15.tif Exists, Skipping\n",
      "23279095_15.tif Exists, Skipping\n",
      "23279140_15.tif Exists, Skipping\n",
      "23279155_15.tif Exists, Skipping\n",
      "23279170_15.tif Exists, Skipping\n",
      "23428900_15.tif Exists, Skipping\n",
      "23428915_15.tif Exists, Skipping\n",
      "23428930_15.tif Exists, Skipping\n",
      "23428945_15.tif Exists, Skipping\n",
      "23428960_15.tif Exists, Skipping\n",
      "23428975_15.tif Exists, Skipping\n",
      "23428990_15.tif Exists, Skipping\n",
      "23429005_15.tif Exists, Skipping\n",
      "23429035_15.tif Exists, Skipping\n",
      "23429050_15.tif Exists, Skipping\n",
      "23429065_15.tif Exists, Skipping\n",
      "23429095_15.tif Exists, Skipping\n",
      "23429125_15.tif Exists, Skipping\n",
      "23429140_15.tif Exists, Skipping\n",
      "23429170_15.tif Exists, Skipping\n",
      "23578915_15.tif Exists, Skipping\n",
      "23578930_15.tif Exists, Skipping\n",
      "23578945_15.tif Exists, Skipping\n",
      "23578975_15.tif Exists, Skipping\n",
      "23578990_15.tif Exists, Skipping\n",
      "23579020_15.tif Exists, Skipping\n",
      "23579035_15.tif Exists, Skipping\n",
      "23579065_15.tif Exists, Skipping\n",
      "23579080_15.tif Exists, Skipping\n",
      "23579095_15.tif Exists, Skipping\n",
      "23579110_15.tif Exists, Skipping\n",
      "23579125_15.tif Exists, Skipping\n",
      "23579140_15.tif Exists, Skipping\n",
      "23728840_15.tif Exists, Skipping\n",
      "23728945_15.tif Exists, Skipping\n",
      "23728960_15.tif Exists, Skipping\n",
      "23728975_15.tif Exists, Skipping\n",
      "23728990_15.tif Exists, Skipping\n",
      "23729005_15.tif Exists, Skipping\n",
      "23729020_15.tif Exists, Skipping\n",
      "23729050_15.tif Exists, Skipping\n",
      "23729065_15.tif Exists, Skipping\n",
      "23729080_15.tif Exists, Skipping\n",
      "23729095_15.tif Exists, Skipping\n",
      "23729110_15.tif Exists, Skipping\n",
      "23878915_15.tif Exists, Skipping\n",
      "23878930_15.tif Exists, Skipping\n",
      "23878945_15.tif Exists, Skipping\n",
      "23878975_15.tif Exists, Skipping\n",
      "23878990_15.tif Exists, Skipping\n",
      "23879020_15.tif Exists, Skipping\n",
      "23879035_15.tif Exists, Skipping\n",
      "23879050_15.tif Exists, Skipping\n",
      "23879065_15.tif Exists, Skipping\n",
      "23879095_15.tif Exists, Skipping\n",
      "23879110_15.tif Exists, Skipping\n",
      "24029035_15.tif Exists, Skipping\n",
      "24029050_15.tif Exists, Skipping\n",
      "24029065_15.tif Exists, Skipping\n",
      "24029080_15.tif Exists, Skipping\n",
      "24029110_15.tif Exists, Skipping\n",
      "24179020_15.tif Exists, Skipping\n",
      "24179035_15.tif Exists, Skipping\n",
      "24179050_15.tif Exists, Skipping\n",
      "24179080_15.tif Exists, Skipping\n",
      "24328840_15.tif Exists, Skipping\n",
      "24328855_15.tif Exists, Skipping\n",
      "24328870_15.tif Exists, Skipping\n",
      "24329020_15.tif Exists, Skipping\n",
      "24329035_15.tif Exists, Skipping\n",
      "24329095_15.tif Exists, Skipping\n",
      "24478840_15.tif Exists, Skipping\n",
      "24478855_15.tif Exists, Skipping\n",
      "24478870_15.tif Exists, Skipping\n",
      "24478885_15.tif Exists, Skipping\n",
      "24478900_15.tif Exists, Skipping\n",
      "24479005_15.tif Exists, Skipping\n",
      "22828930_15.tif Exists, Skipping\n",
      "22828990_15.tif Exists, Skipping\n",
      "22829050_15.tif Exists, Skipping\n",
      "23429020_15.tif Exists, Skipping\n",
      "23429080_15.tif Exists, Skipping\n",
      "23578960_15.tif Exists, Skipping\n",
      "23579005_15.tif Exists, Skipping\n",
      "23729035_15.tif Exists, Skipping\n",
      "23879080_15.tif Exists, Skipping\n",
      "24179065_15.tif Exists, Skipping\n",
      "22978945_15.tif Exists, Skipping\n",
      "23429155_15.tif Exists, Skipping\n",
      "23579050_15.tif Exists, Skipping\n",
      "23728930_15.tif Exists, Skipping\n"
     ]
    }
   ],
   "source": [
    "massachussets.download_images()\n",
    "massachussets.download_masks()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISPRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISPRSSemanticLabelingETL(DatasetETL):\n",
    "    isprs_urls = {\n",
    "        \"potsdam.zip\": \"https://seafile.projekt.uni-hannover.de/f/429be50cc79d423ab6c4/\",\n",
    "        \"toronto.zip\": \"https://seafile.projekt.uni-hannover.de/f/fc62f9c20a8c4a34aea1/\",\n",
    "        \"vaihingen.zip\": \"https://seafile.projekt.uni-hannover.de/f/6a06a837b1f349cfa749/\",\n",
    "    }\n",
    "    password = \"CjwcipT4-P8g\"\n",
    "    cookie_name = \"sfcsrftoken\"\n",
    "\n",
    "    def __init__(self, root:Path, low_storage_mode:bool = True):\n",
    "        super().__init__(root = root, \n",
    "                         source_urls = self.isprs_urls, \n",
    "                         low_storage_mode = low_storage_mode\n",
    "        )\n",
    "\n",
    "    def _download_file(self, file_path:str, url:str, chunk_size = 1024*1024) -> None:\n",
    "        session = requests.Session()\n",
    "        cookies = {self.cookie_name: session.get(url).cookies.get(self.cookie_name)}\n",
    "        payload:dict = {'csrfmiddlewaretoken': cookies[self.cookie_name], \n",
    "                        'password': self.password}\n",
    "        with requests.post(url+\"?dl=1\", data = payload, cookies = cookies, stream = True) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            with open(file_path, \"wb\") as f, tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk: \n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "    \n",
    "    def download(self):\n",
    "        self.download_source_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "isprs = ISPRSSemanticLabelingETL(ROOT / \"isprs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   9%|▊         | 1.16G/13.3G [16:28<2:52:33, 1.17MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m isprs\u001b[39m.\u001b[39;49mdownload()\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mISPRSSemanticLabelingETL.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m location, url \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misprs_urls\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     28\u001b[0m     zip_file_path:Path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlocation\u001b[39m}\u001b[39;00m\u001b[39m.zip\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_file(zip_file_path, url)\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mISPRSSemanticLabelingETL.download_file\u001b[1;34m(self, file_path, url, chunk_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m total_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m     20\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f, tqdm(total\u001b[39m=\u001b[39mtotal_size, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m progress_bar:\n\u001b[1;32m---> 21\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mchunk_size):\n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m chunk: \n\u001b[0;32m     23\u001b[0m             f\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "isprs.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityOSMETL(DatasetETL):\n",
    "    city_osm_urls = {\n",
    "        \"Berlin.zip\": \"https://zenodo.org/record/1154821/files/berlin.zip?download=1\",\n",
    "        \"Chicago.zip\": \"https://zenodo.org/record/1154821/files/chicago.zip?download=1\",\n",
    "        \"Paris.zip\": \"https://zenodo.org/record/1154821/files/paris.zip?download=1\",\n",
    "        \"Potsdam.zip\": \"https://zenodo.org/record/1154821/files/potsdam.zip?download=1\",\n",
    "        \"Tokyo.zip\": \"https://zenodo.org/record/1154821/files/tokyo.zip?download=1\",\n",
    "        \"Zurich.zip\": \"https://zenodo.org/record/1154821/files/zurich.zip?download=1\"\n",
    "    }\n",
    "    def __init__(self, root:Path, low_storage_mode:bool = True):\n",
    "        super().__init__(root, \n",
    "                         self.city_osm_urls, \n",
    "                         low_storage_mode=low_storage_mode)\n",
    "    \n",
    "    def download(self):\n",
    "        self.download_source_urls()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_osm = CityOSMETL(ROOT / \"city-osm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:  13%|█▎        | 261M/2.00G [25:31<2:50:26, 170kB/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m city_osm\u001b[39m.\u001b[39;49mdownload()\n",
      "Cell \u001b[1;32mIn[20], line 15\u001b[0m, in \u001b[0;36mCityOSMETL.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m file_name, url \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcity_osm_urls\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 15\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_file(url, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot_dir \u001b[39m/\u001b[39;49m (file_name\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.zip\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mDatasetETL.download_file\u001b[1;34m(self, url, file_path, chunk_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m total_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m     13\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f, tqdm(total\u001b[39m=\u001b[39mtotal_size, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m progress_bar:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mchunk_size):\n\u001b[0;32m     15\u001b[0m         \u001b[39mif\u001b[39;00m chunk: \n\u001b[0;32m     16\u001b[0m             f\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "city_osm.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceNetETL(DatasetETL):\n",
    "    \n",
    "    def __init__(self, root:Path):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
