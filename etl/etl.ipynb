{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import boto3\n",
    "\n",
    "import zipfile\n",
    "import py7zr\n",
    "import multivolumefile\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"C:/Users/hp/Desktop/datasets/urban-feature-extraction\")\n",
    "ROOT = Path(\"C:/Users/hp/Desktop/urban-feature-extraction\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetETL:\n",
    "    def __init__(self, root:Path, urls:dict, image_dir:Path, mask_dir:Path, low_storage_mode:bool = True):\n",
    "        download_dir = (root / \"downloads\") \n",
    "        if not root.exists():\n",
    "            print(\"Root not found, creating directories\")\n",
    "            root.mkdir()\n",
    "            download_dir.mkdir()\n",
    "        self.root = root\n",
    "        #Source URLs is Dict[file_name:str, url:str]\n",
    "        self.source_urls:dict = urls\n",
    "\n",
    "        self.low_storage_mode = low_storage_mode\n",
    "        self.random_seed = 11235 \n",
    "\n",
    "        if not download_dir.exists():\n",
    "            print(\"Downloads directory DNE, creating new directory\")\n",
    "            download_dir.mkdir()\n",
    "        self.download_dir = download_dir \n",
    "\n",
    "        (image_dir).mkdir(exist_ok=True, parents=True)\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        (mask_dir).mkdir(exist_ok=True, parents=True) \n",
    "        self.mask_dir = mask_dir\n",
    "\n",
    "        self.cropped_image_dir = self.root / \"cropped\" / \"images\"\n",
    "        (self.cropped_image_dir).mkdir(exist_ok=True)\n",
    "        self.cropped_mask_dir = self.root / \"cropped\" / \"masks\"\n",
    "        (self.cropped_mask_dir).mkdir(exist_ok=True)\n",
    "        self.cropped_metadata_dir = self.root / \"cropped\" / \"metadata\"\n",
    "        (self.cropped_metadata_dir).mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "    #TODO Raise error if these are not implemented in child function\n",
    "    def download(self):\n",
    "        pass\n",
    "    def extract(self):\n",
    "        pass\n",
    "    def catalog(self):\n",
    "        pass\n",
    "\n",
    "    def Extract(self):\n",
    "        #Download and Stage Dataset on Disk.\n",
    "        self.download()\n",
    "        self.extract()\n",
    "        self.catalog() \n",
    "\n",
    "    def Transform(self):\n",
    "        pass\n",
    "    def Load(self):\n",
    "        pass\n",
    "\n",
    "    def clear_downloads_directory(self):\n",
    "        downloaded_files:list = [path for path in self.download_dir.iterdir()]\n",
    "        self.delete_files(downloaded_files)\n",
    "\n",
    "    #Internal Methods\n",
    "\n",
    "    def _get_source_urls(self, urls_list:list) -> dict:\n",
    "        #List[url:str] -> Dict[file_name:str, url:str]\n",
    "\n",
    "        return {Path(url).name : url for url in urls_list}\n",
    "\n",
    "    def _download_file(self, url:str, file_path:Path, chunk_size:int = 1024*1024):\n",
    "        #Download from url and save to disk at file_path\n",
    "        \n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            with open(file_path, \"wb\") as f, tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk: \n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "\n",
    "    #TODO: Use async/await for concurrent downloads \n",
    "    def _download_source_urls(self, download_dir:Path = Path(\"\")):\n",
    "        if download_dir == Path(\"\"):\n",
    "            download_dir = self.download_dir\n",
    "        #Download files from self.source_urls, skip if already_downloaded\n",
    "        print(download_dir)\n",
    "        for file_name, url in self.source_urls.items():\n",
    "            file_path = download_dir / file_name \n",
    "\n",
    "            if file_path.exists():\n",
    "                #TODO: Check for downloaded file size as well\n",
    "                print(f\"{file_name} Already Downloaded, Skipping\")\n",
    "                continue\n",
    "            self._download_file(url, file_path)\n",
    "\n",
    "    def _extract_zip(self, zip_file_path:Path, target_dir:Path, dirs_to_be_extracted = list()):\n",
    "        #Extract specified dirs from zip archive, extract all dirs if not specified\n",
    "\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip:\n",
    "            #If dirs_to_be_extracted is an empty list, extract entire archive and exit\n",
    "            if not dirs_to_be_extracted:\n",
    "                zip.extractall(target_dir); return\n",
    "            #Otherwise, extract all files under specified dirs\n",
    "            #For each file in archive, extract if it's under any specified dir\n",
    "            for member in zip.infolist():\n",
    "                for foldername in dirs_to_be_extracted:\n",
    "                    if foldername in member.filename:\n",
    "                        #TODO: Add tqdm progress bar for extraction\n",
    "                        zip.extract(member, target_dir)\n",
    "   \n",
    "    def _validate_files(self, dir:Path, files:list) -> list:\n",
    "        ##TODO: Check for downloaded file size on disk against actual size\n",
    "        missing = list()\n",
    "        for file_name in files:\n",
    "            if not(dir/file_name).exists():\n",
    "                missing.append(file_name)\n",
    "        return missing\n",
    "\n",
    "    def _validate_download(self) -> list:\n",
    "        return self._validate_files(self.download_dir, self.source_urls.keys())\n",
    "        \n",
    "    def _delete_files(self, file_paths:list):\n",
    "        #Delete list of files if they exist, print warnings if they dont. \n",
    "        for file_path in file_paths:\n",
    "            if file_path.exists():\n",
    "                file_path.unlink()\n",
    "            else:\n",
    "                print(f\"Error Deleting {file_path.name}\")\n",
    "                print(f\"File Does Not Exist\")\n",
    "\n",
    "    def _get_raster_metadata(self, raster_path:Path):\n",
    "        #Return Shape, Reference Frame and Transformation Matrix of a Raster File\n",
    "        with rio.open(raster_path) as raster:\n",
    "            return raster.shape, str(raster.crs), tuple(raster.transform)\n",
    "    \n",
    "    def _complete_catalog(self, df:pd.DataFrame) -> None:\n",
    "        \"\"\"Add metadata columns to catalog and assign self.downloaded_dataset\"\"\"\n",
    "\n",
    "        metadata = df.name.apply(lambda x: self._get_raster_metadata((self.mask_dir/x)))\n",
    "        df[[\"shape\", \"crs\", \"transform\"]] = pd.DataFrame(metadata.tolist(), index = df.index) \n",
    "        #df[[\"shape\", \"crs\", \"transform\"]] = metadata.apply(pd.Series) \n",
    "        #df[\"image_path\"] = df.name.apply(lambda x: self.image_dir / x)\n",
    "        #df[\"mask_path\"] = df.name.apply(lambda x: self.mask_dir / x)\n",
    "        self.downloaded_dataset = df.copy(deep = True)\n",
    "\n",
    "    def _read_image(self, path):\n",
    "        return skimage.io.imread(path) \n",
    "\n",
    "    def _read_mask(self, path):\n",
    "        return skimage.io.imread(path) \n",
    "\n",
    "    def _get_pad_amount(self, dimension: int, window: int):\n",
    "        \"\"\"Calculate to no of pixels to add to before and after dimension\"\"\"\n",
    "        total_padding = window - (dimension % window)\n",
    "\n",
    "        if total_padding % 2 == 0:\n",
    "            after = total_padding // 2\n",
    "            before = after\n",
    "        else:\n",
    "            after = (total_padding // 2) + 1\n",
    "            before = after - 1\n",
    "        assert before+after == total_padding \n",
    "        return (before, after)\n",
    "    \n",
    "    def _pad_3d_array(self, array: np.ndarray, window: int):\n",
    "        \"\"\"\n",
    "        Pad image array s.t. divisible by window\\n\n",
    "        array.shape : (Height, Width, Channels)\n",
    "        window : side length of square cropping window\n",
    "        \"\"\"\n",
    "\n",
    "        assert array.ndim == 3\n",
    "        padded_array = np.pad(\n",
    "            array = array,\n",
    "            pad_width = (self._get_pad_amount(array.shape[0], window),\n",
    "                         self._get_pad_amount(array.shape[1], window),\n",
    "                         (0, 0))\n",
    "        ) \n",
    "        return padded_array\n",
    "    \n",
    "    def _get_cropped_view(self, array: np.ndarray, window:int):\n",
    "        \"\"\"\n",
    "        Crop image array s.t. divisible by window\\n\n",
    "        array.shape : (Height, Width, Channels)\n",
    "        window : side length of square cropping window\n",
    "        \"\"\"\n",
    "\n",
    "        assert array.ndim == 3\n",
    "        cropped_view = skimage.util.view_as_windows(\n",
    "            arr_in = array,\n",
    "            window_shape = (window, window, array.shape[2]),\n",
    "            step =  (window, window, array.shape[2])).squeeze()\n",
    "            \n",
    "        cropped_view = cropped_view.reshape(-1, window, window, array.shape[2])\n",
    "\n",
    "        return cropped_view\n",
    "\n",
    "    def _crop_one_scene(self, tile_path: str, window: int, read_scene):\n",
    "        scene = read_scene(tile_path) \n",
    "        scene = self._pad_3d_array(scene, window)\n",
    "        scene = self._get_cropped_view(scene, window)\n",
    "        return scene\n",
    "\n",
    "    def _save_as_jpeg_100(self, array: np.ndarray, out_path: Path) -> None:\n",
    "        skimage.io.imsave((out_path.parent / f\"{out_path.stem}.jpg\"), array, check_contrast = False, **{\"quality\": 100})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InriaETL(DatasetETL):\n",
    "    def __init__(self, root:Path, low_storage_mode:bool = True):\n",
    "\n",
    "        self.locations = [\"austin\", \"chicago\", \"kitsap\", \"tyrol-w\", \"vienna\"]\n",
    "        self.files_list = [f\"{location}{num}.tif\" for location in self.locations for num in range(1, 37)]\n",
    "        dataset_dir = root / \"AerialImageDataset\" / \"train\"\n",
    "\n",
    "        super().__init__(\n",
    "            root = root, \n",
    "            urls = self._get_source_urls([\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.001\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.002\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.003\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.004\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.005\"\n",
    "                                        ]),\n",
    "            image_dir = dataset_dir / \"images\",\n",
    "            mask_dir = dataset_dir / \"gt\",\n",
    "            low_storage_mode = low_storage_mode\n",
    "        )\n",
    "\n",
    "    def download(self):\n",
    "        self._download_source_urls(self.download_dir)\n",
    "    \n",
    "    def extract(self):\n",
    "        #Verify Download\n",
    "        missing_volumes = self._validate_download()\n",
    "        if missing_volumes:\n",
    "            print(\"Missing Volumes\")\n",
    "            print(\"Please Download Missing Volumes\")\n",
    "            print(missing_volumes)\n",
    "            return\n",
    "        else:\n",
    "            print(\"Found All Volumes\")\n",
    "\n",
    "\n",
    "        #Merge and Extract Dataset Zip\n",
    "        multivolume_7zip_path = self.download_dir / \"aerialimagelabeling.7z\" \n",
    "        self._extract_multivolume_archive(multivolume_7zip_path, self.download_dir)\n",
    "\n",
    "        #Delete downloaded volumes        \n",
    "        if self.low_storage_mode:\n",
    "            print(\"Deleting downloaded volumes to save storage space\")\n",
    "            #self._delete_files(self.download_dir.glob(\"*.7z.*\"))\n",
    "        \n",
    "        dataset_zip_path = self.download_dir / \"NEW2-AerialImageDataset.zip\" \n",
    "        print(f\"Extracting Dataset Folder from {dataset_zip_path}\")\n",
    "        self._extract_zip(dataset_zip_path, self.root, [\"train\"])\n",
    "\n",
    "        missing_images, missing_masks = self._validate_extraction()\n",
    "        if missing_images or missing_masks:\n",
    "            print(f\"Images Not Found: {missing_images}\")\n",
    "            print(f\"Masks Not Found: {missing_masks}\")\n",
    "        else:\n",
    "            print(\"Extraction Complete\")\n",
    "            print(\"Deleting dataset archive\")\n",
    "            self._delete_files([dataset_zip_path])\n",
    "\n",
    "    def catalog(self):\n",
    "        df = pd.DataFrame({\"name\": self.files_list,\n",
    "                           \"split\": list(map(self._get_split, self.files_list))})\n",
    "        self._complete_catalog(df)\n",
    "\n",
    "    def crop(self, window: int):\n",
    "        for (idx, tile) in self.downloaded_dataset.iterrows():\n",
    "\n",
    "            if tile[\"split\"] == \"test\":\n",
    "                continue\n",
    "\n",
    "            cropped_image_view = self._crop_one_scene(\n",
    "                tile_path = self.image_dir / tile[\"name\"],\n",
    "                window = window,\n",
    "                read_scene = self._read_image\n",
    "            )\n",
    "\n",
    "            cropped_mask_view = self._crop_one_scene(\n",
    "                tile_path = self.mask_dir / tile[\"name\"],\n",
    "                window = window,\n",
    "                read_scene = self._read_mask\n",
    "            )\n",
    "    \n",
    "            for image_crop, mask_crop in zip(cropped_image_view, cropped_mask_view):\n",
    "                crop_name = str(uuid.uuid4())\n",
    "                self._save_as_jpeg_100(image_crop , (self.cropped_image_dir / crop_name)) \n",
    "                self._save_as_jpeg_100(mask_crop.squeeze(), (self.cropped_mask_dir / crop_name))\n",
    "\n",
    "    #Internal Methods\n",
    "    def _extract_multivolume_archive(self, multivolume_file_path:Path, target_dir:Path) -> None:\n",
    "        \"\"\"Extract contents of a multivolume 7zip archive\"\"\" \n",
    "\n",
    "        with multivolumefile.open(multivolume_file_path, mode = 'rb') as multi_archive:\n",
    "            with py7zr.SevenZipFile(multi_archive, 'r') as archive:\n",
    "                archive.extractall(path = target_dir)\n",
    "    \n",
    "    def _validate_extraction(self) -> tuple:\n",
    "        return (self._validate_files(self.image_dir, self.files_list),\n",
    "                self._validate_files(self.mask_dir, self.files_list))\n",
    "\n",
    "    def _get_split(self, file_name:str):\n",
    "        \"\"\"First 6 (16.67%) in every region for testing \"\"\"\n",
    "        numbers = [char for char in file_name if char.isdigit()]\n",
    "        if int(''.join(numbers)) <= 6:\n",
    "            return \"test\"\n",
    "        return \"train\"\n",
    "    \n",
    "    def _read_mask(self, path:str):\n",
    "        mask = skimage.io.imread(path) \n",
    "        return np.expand_dims(mask, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria = InriaETL(DATA / \"inria\")\n",
    "inria.catalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria.crop(512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massachussets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassachussetsETL(DatasetETL):\n",
    "    mass_urls = {\n",
    "        \"train\": (\"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/train/sat/index.html\", \"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/train/map/index.html\"),\n",
    "        \"test\" : (\"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/test/sat/index.html\", \"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/test/map/index.html\"),\n",
    "        \"val\": (\"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/valid/sat/index.html\", \"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/valid/map/index.html\"),\n",
    "    }\n",
    "    def __init__(self, root:Path):\n",
    "        super().__init__(root, self._get_source_urls())\n",
    "        self.image_dir = self.root / \"images\"\n",
    "        self.image_dir.mkdir(exist_ok=True)\n",
    "        self.mask_dir = self.root / \"masks\"\n",
    "        self.mask_dir.mkdir(exist_ok=True)\n",
    "        self.files_list = [url.split(\"/\")[-1] for url in self.source_urls[0]]\n",
    "\n",
    "    def _get_file_urls(self, url) -> list:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            pattern = r'<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"'\n",
    "            matches = re.findall(pattern, response.text)\n",
    "            return matches\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code)\n",
    "            return []\n",
    "\n",
    "    def _get_source_urls(self) -> tuple:\n",
    "        images = sum([self._get_file_urls(self.mass_urls[key][0]) for key in self.mass_urls.keys()], [])\n",
    "        masks = sum([self._get_file_urls(self.mass_urls[key][1]) for key in self.mass_urls.keys()], [])\n",
    "        return images, masks\n",
    "\n",
    "    def download(self, urls:list, target_dir_path:Path):\n",
    "        for url in urls:\n",
    "            downloaded_file_path = target_dir_path / url.split(\"/\")[-1]\n",
    "            if downloaded_file_path.exists():\n",
    "                print(f\"{url.split('/')[-1]} Exists, Skipping\")\n",
    "                continue\n",
    "            self.download_file(url, downloaded_file_path)\n",
    "    \n",
    "    def download_images(self) -> None:\n",
    "        self.download(self.source_urls[0], self.image_dir)\n",
    "        \n",
    "    def download_masks(self) -> None:\n",
    "        self.download(self.source_urls[1], self.image_dir)\n",
    "        \n",
    "    def _validate_download_dir(self, target_dir_path:Path) -> list:\n",
    "        files_not_downloaded = list()\n",
    "        for file_name in self.files_list:\n",
    "            downloaded_file_path = target_dir_path / file_name\n",
    "            if not downloaded_file_path.exists():\n",
    "                files_not_downloaded.append(downloaded_file_path)\n",
    "        return files_not_downloaded\n",
    "    \n",
    "    def validate_images(self) -> None:\n",
    "        return self._validate_download_dir(self.image_dir)\n",
    "\n",
    "    def validate_masks(self) -> None:\n",
    "        return self._validate_download_dir(self.mask_dir)\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        metadata = [self.get_raster_metadata((self.mask_dir / x)) for x in self.files_list] \n",
    "        shapes, crses, transforms = zip(*metadata)\n",
    "        data = {\n",
    "            \"name\": self.files_list,\n",
    "            \"shape\": shapes, \n",
    "            \"crs\": crses,\n",
    "            \"transform\": transforms,\n",
    "            \"image_path\": [(self.image_dir / x) for x in self.files_list],\n",
    "            \"mask_path\": [(self.mask_dir / x) for x in self.files_list]\n",
    "        }\n",
    "        return pd.DataFrame(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISPRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISPRSSemanticLabelingETL(DatasetETL):\n",
    "    isprs_urls = {\n",
    "        \"potsdam.zip\": \"https://seafile.projekt.uni-hannover.de/f/429be50cc79d423ab6c4/\",\n",
    "        #\"toronto.zip\": \"https://seafile.projekt.uni-hannover.de/f/fc62f9c20a8c4a34aea1/\",\n",
    "        \"vaihingen.zip\": \"https://seafile.projekt.uni-hannover.de/f/6a06a837b1f349cfa749/\",\n",
    "    }\n",
    "    password = \"CjwcipT4-P8g\"\n",
    "    cookie_name = \"sfcsrftoken\"\n",
    "\n",
    "    def __init__(self, root:Path, low_storage_mode:bool = True):\n",
    "        super().__init__(root = root, \n",
    "                         urls = self.isprs_urls, \n",
    "                         image_dir = root / \"images\",\n",
    "                         mask_dir = root / \"masks\"\n",
    "        )\n",
    "\n",
    "    def _download_file(self, url:str, file_path:str, chunk_size = 1024*1024) -> None:\n",
    "        session = requests.Session()\n",
    "        cookies = {self.cookie_name: session.get(url).cookies.get(self.cookie_name)}\n",
    "        payload:dict = {'csrfmiddlewaretoken': cookies[self.cookie_name], \n",
    "                        'password': self.password}\n",
    "        with requests.post(url+\"?dl=1\", data = payload, cookies = cookies, stream = True) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            with open(file_path, \"wb\") as f, tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk: \n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "\n",
    "    def download(self):\n",
    "        self._download_source_urls() \n",
    "    \n",
    "    def extract(self):\n",
    "        self._extract_vaihingen()\n",
    "        self._extract_potsdam()\n",
    "    \n",
    "    def catalog(self):\n",
    "        mask_file_names = set([x.name for x in self.mask_dir.glob(\"*.tif\")])\n",
    "        image_file_names = set([x.name for x in self.image_dir.glob(\"*.tif\")])\n",
    "        print(mask_file_names - image_file_names)\n",
    "\n",
    "    def _extract_vaihingen(self):\n",
    "        vaihingen_zip_path = self.download_dir / \"vaihingen.zip\"\n",
    "        dataset_zip_path = self.download_dir / \"Vaihingen\" / \"ISPRS_semantic_labeling_Vaihingen.zip\"\n",
    "        self._extract_zip(vaihingen_zip_path, self.download_dir, [dataset_zip_path.name])\n",
    "        if dataset_zip_path.exists():\n",
    "            print(\"Dataset Zip Extracted\")\n",
    "        if self.low_storage_mode:\n",
    "            #self._delete_files([vaihingen_zip_path])\n",
    "            print(\"Downloaded Zip Deleted\")\n",
    "\n",
    "        self._extract_zip(dataset_zip_path, self.root, [\"top\", \"gts_for_participants\"])\n",
    "        print(\"Dataset Extracted\")\n",
    "        if self.low_storage_mode:\n",
    "            #self._delete_files([dataset_zip_path])\n",
    "            #dataset_zip_path.parent.rmdir()\n",
    "            print(\"Dataset Zip Deleted\")\n",
    "        shutil.move(self.root / \"top\", self.image_dir)\n",
    "        shutil.move(self.root / \"gts_for_participants\", self.mask_dir)\n",
    "    \n",
    "    def _extract_potsdam(self):\n",
    "        potsdam_zip_path = self.download_dir / \"potsdam.zip\"\n",
    "        images_zip_path = self.download_dir / \"Potsdam\" / \"2_Ortho_RGB.zip\"\n",
    "        masks_zip_path = self.download_dir / \"Potsdam\" / \"5_Labels_all.zip\"\n",
    "\n",
    "        #self._extract_zip(potsdam_zip_path, self.download_dir, [images_zip_path.name, masks_zip_path.name])\n",
    "\n",
    "        images_temp_dir = images_zip_path.parent / images_zip_path.stem\n",
    "        #self._extract_zip(images_zip_path, images_zip_path.parent)\n",
    "\n",
    "        masks_temp_dir = masks_zip_path.parent / masks_zip_path.stem\n",
    "        (masks_temp_dir).mkdir(exist_ok=True)\n",
    "        #self._extract_zip(masks_zip_path, masks_temp_dir)\n",
    "\n",
    "        #Copy Images, Masks to Correct Directories\n",
    "        for image_path in images_temp_dir.glob(\"*.tif\"):\n",
    "            shutil.move(image_path, self.image_dir)\n",
    "        for mask_path in masks_temp_dir.glob(\"*.tif\"):\n",
    "            shutil.move(mask_path, self.mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "isprs = ISPRSSemanticLabelingETL(DATA / \"isprs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top_potsdam_2_12_label.tif', 'top_potsdam_6_13_label.tif', 'top_potsdam_4_12_label.tif', 'top_potsdam_3_13_label.tif', 'top_potsdam_6_9_label.tif', 'top_potsdam_4_13_label.tif', 'top_potsdam_6_12_label.tif', 'top_potsdam_7_8_label.tif', 'top_potsdam_3_11_label.tif', 'top_potsdam_5_15_label.tif', 'top_potsdam_4_15_label.tif', 'top_potsdam_4_10_label.tif', 'top_potsdam_5_10_label.tif', 'top_potsdam_7_9_label.tif', 'top_potsdam_3_10_label.tif', 'top_potsdam_6_11_label.tif', 'top_potsdam_3_14_label.tif', 'top_potsdam_7_7_label.tif', 'top_potsdam_5_12_label.tif', 'top_potsdam_7_10_label.tif', 'top_potsdam_2_14_label.tif', 'top_potsdam_4_11_label.tif', 'top_potsdam_7_13_label.tif', 'top_potsdam_7_12_label.tif', 'top_potsdam_6_8_label.tif', 'top_potsdam_5_11_label.tif', 'top_potsdam_6_14_label.tif', 'top_potsdam_6_7_label.tif', 'top_potsdam_5_14_label.tif', 'top_potsdam_7_11_label.tif', 'top_potsdam_2_13_label.tif', 'top_potsdam_5_13_label.tif', 'top_potsdam_4_14_label.tif', 'top_potsdam_6_15_label.tif', 'top_potsdam_2_10_label.tif', 'top_potsdam_2_11_label.tif', 'top_potsdam_6_10_label.tif', 'top_potsdam_3_12_label.tif'}\n"
     ]
    }
   ],
   "source": [
    "isprs.catalog()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityOSMETL(DatasetETL):\n",
    "    city_osm_urls = {\n",
    "        \"berlin.zip\": \"https://zenodo.org/record/1154821/files/berlin.zip?download=1\",\n",
    "        \"chicago.zip\": \"https://zenodo.org/record/1154821/files/chicago.zip?download=1\",\n",
    "        \"paris.zip\": \"https://zenodo.org/record/1154821/files/paris.zip?download=1\",\n",
    "        \"potsdam.zip\": \"https://zenodo.org/record/1154821/files/potsdam.zip?download=1\",\n",
    "        \"tokyo.zip\": \"https://zenodo.org/record/1154821/files/tokyo.zip?download=1\",\n",
    "        \"zurich.zip\": \"https://zenodo.org/record/1154821/files/zurich.zip?download=1\"\n",
    "    }\n",
    "    def __init__(self, root:Path):\n",
    "        super().__init__(root, \n",
    "                         self.city_osm_urls, \n",
    "                         image_dir = root / \"images\", \n",
    "                         mask_dir = root / \"masks\")\n",
    "    \n",
    "    def download(self):\n",
    "        self._download_source_urls()\n",
    "    \n",
    "    def extract(self):\n",
    "        #Extract All Files\n",
    "        for zip_file_name in self.source_urls.keys():\n",
    "            zip_file_path = self.download_dir / zip_file_name\n",
    "            self._extract_zip(zip_file_path, self.image_dir, [\"image\"])\n",
    "            self._extract_zip(zip_file_path, self.mask_dir, [\"labels\"])\n",
    "\n",
    "        self._move_and_rename_files(self.image_dir.rglob(\"*.png\"))\n",
    "        self._move_and_rename_files(self.mask_dir.rglob(\"*.png\"))\n",
    "\n",
    "        #TODO: Remove Empty Directories\n",
    "        #for dir_path in self.image_dir.glob('**/'):\n",
    "            #if dir_path.exists() and not any(dir_path.iterdir()):\n",
    "                #dir_path.rmdir()\n",
    "        #for dir_path in self.mask_dir.glob('**/'):\n",
    "            #if dir_path.exists() and not any(dir_path.iterdir()):\n",
    "                #dir_path.rmdir()\n",
    "\n",
    "    def catalog(self):\n",
    "        self.file_names = list(map(lambda x: x.name, self.mask_dir.rglob(\"*.png\")))\n",
    "        df = pd.DataFrame({\"name\": self.file_names})\n",
    "        df[\"split\"] = df.apply(lambda x: self._get_split(), axis = 1) \n",
    "        self._complete_catalog(df)\n",
    "        \n",
    "    def _move_and_rename_files(self, file_paths) -> None:\n",
    "        for file_path in file_paths:\n",
    "            shutil.move(file_path, file_path.parents[1] / f\"{file_path.stem.split('_')[0]}.png\")\n",
    "    \n",
    "    def _get_split(self):\n",
    "        \"\"\"Train-Test Split with 15% Probability\"\"\"\n",
    "        return np.random.choice([\"train\", \"test\"], p = [.84, .16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_osm = CityOSMETL(DATA / \"city-osm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
