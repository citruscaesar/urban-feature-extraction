{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import boto3\n",
    "\n",
    "import zipfile\n",
    "import py7zr\n",
    "import multivolumefile\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"C:/Users/hp/Desktop/datasets/urban-feature-extraction\")\n",
    "ROOT = Path(\"C:/Users/hp/Desktop/urban-feature-extraction\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetETL:\n",
    "    def __init__(self, root:Path, urls:dict, low_storage_mode:bool = True, image_dir:Path = Path(\"\"), mask_dir:Path = Path(\"\")):\n",
    "        download_dir = (root / \"downloads\") \n",
    "        if not root.exists():\n",
    "            print(\"Root not found, creating directories\")\n",
    "            root.mkdir()\n",
    "            download_dir.mkdir()\n",
    "        self.root_dir = root\n",
    "        self.low_storage_mode = low_storage_mode\n",
    "\n",
    "        if not download_dir.exists():\n",
    "            print(\"Downloads directory DNE, creating new directory\")\n",
    "            download_dir.mkdir()\n",
    "        self.download_dir = download_dir \n",
    "\n",
    "        if not image_dir == Path(\"\"):\n",
    "            (image_dir).mkdir(exist_ok=True, parents=True)\n",
    "            self.image_dir = image_dir\n",
    "\n",
    "        if not mask_dir == Path(\"\"):\n",
    "            (mask_dir).mkdir(exist_ok=True, parents=True) \n",
    "            self.mask_dir = mask_dir\n",
    "\n",
    "        #Source URLs is Dict[file_name:str, url:str]\n",
    "        self.source_urls:dict = urls\n",
    "\n",
    "    #TODO Raise error if these are not implemented in child function\n",
    "    def download(self):\n",
    "        pass\n",
    "    def extract(self):\n",
    "        pass\n",
    "    def catalog(self):\n",
    "        pass\n",
    "\n",
    "    def Extract(self):\n",
    "        #Download and Stage Dataset on Disk.\n",
    "        self.download()\n",
    "        self.extract()\n",
    "        self.catalog() \n",
    "\n",
    "    def Transform(self):\n",
    "        pass\n",
    "    def Load(self):\n",
    "        pass\n",
    "\n",
    "    def clear_downloads_directory(self):\n",
    "        downloaded_files:list = [path for path in self.download_dir.iterdir()]\n",
    "        self.delete_files(downloaded_files)\n",
    "\n",
    "    #Internal Methods\n",
    "\n",
    "    def _get_source_urls(self, urls_list:list) -> dict:\n",
    "        #List[url:str] -> Dict[file_name:str, url:str]\n",
    "\n",
    "        return {Path(url).name : url for url in urls_list}\n",
    "\n",
    "    def _download_file(self, url:str, file_path:Path, chunk_size:int = 1024*1024):\n",
    "        #Download from url and save to disk at file_path\n",
    "\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            with open(file_path, \"wb\") as f, tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk: \n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "\n",
    "    def _download_source_urls(self, download_dir:Path = Path(\"\")):\n",
    "        if download_dir == Path(\"\"):\n",
    "            download_dir = self.download_dir\n",
    "        #Download files from self.source_urls, skip if already_downloaded\n",
    "\n",
    "        for file_name, url in self.source_urls.items():\n",
    "            file_path = download_dir / file_name \n",
    "\n",
    "            if file_path.exists():\n",
    "                #TODO: Check for downloaded file size as well\n",
    "                print(f\"{file_name} Already Downloaded, Skipping\")\n",
    "                continue\n",
    "            self._download_file(url, file_path)\n",
    "\n",
    "    def _extract_zip(self, zip_file_path:Path, target_dir:Path, dirs_to_be_extracted = list()):\n",
    "        #Extract specified dirs from zip archive, extract all dirs if not specified\n",
    "\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip:\n",
    "            #If dirs_to_be_extracted is an empty list, extract entire archive and exit\n",
    "            if not dirs_to_be_extracted:\n",
    "                zip.extractall(target_dir); return\n",
    "            #Otherwise, extract all files under specified dirs\n",
    "            #For each file in archive, extract if it's under any specified dir\n",
    "            for member in zip.infolist():\n",
    "                for foldername in dirs_to_be_extracted:\n",
    "                    if foldername in member.filename:\n",
    "                        #TODO: Add tqdm progress bar for extraction\n",
    "                        zip.extract(member, target_dir)\n",
    "\n",
    "    def _extract_7zip(self, zip_file_path:Path, target_dir:Path, dirs_to_be_extracted = list()):\n",
    "        #Extract specified dirs from 7zip archive, extract all dirs if not specified\n",
    "\n",
    "        with py7zr.SevenZipFile(zip_file_path, 'r') as zip:\n",
    "\n",
    "        #If dirs_to_be_extracted is an empty list, extract entire archive\n",
    "            if not dirs_to_be_extracted: \n",
    "                zip.extractall(target_dir); return\n",
    "\n",
    "        #Otherwise, extract all files under specified dirs\n",
    "        #For each file in archive, extract if it's under any specified dir\n",
    "            for member in zip.getnames():\n",
    "                for foldername in dirs_to_be_extracted:\n",
    "                    if foldername in member:\n",
    "                        zip.extract(target_dir, member)\n",
    "                        zip.reset()\n",
    "\n",
    "    def _merge_multivolume_archive(self, multivolume_paths:list, target_zip_path:Path):\n",
    "        #Combine multivolume archive files into a single archive\n",
    "\n",
    "        with open(target_zip_path, 'ab') as outfile:\n",
    "            for volume_path in multivolume_paths:\n",
    "                with open(volume_path, 'rb') as infile:\n",
    "                    outfile.write(infile.read())\n",
    "    \n",
    "    def _validate_files(self, dir:Path, files:list) -> list:\n",
    "        ##TODO: Check for downloaded file size on disk against actual size\n",
    "        missing = list()\n",
    "        for file_name in files:\n",
    "            if not(dir/file_name).exists():\n",
    "                missing.append(file_name)\n",
    "        return missing\n",
    "\n",
    "    def _validate_download(self) -> list:\n",
    "        return self._validate_files(self.download_dir, self.source_urls.keys())\n",
    "        \n",
    "    def _delete_files(self, file_paths:list):\n",
    "        #Delete list of files if they exist, print warnings if they dont. \n",
    "        for file_path in file_paths:\n",
    "            if file_path.exists():\n",
    "                file_path.unlink()\n",
    "            else:\n",
    "                print(f\"Error Deleting {file_path.name}\")\n",
    "                print(f\"File Does Not Exist\")\n",
    "\n",
    "    def _get_raster_metadata(self, raster_path:Path):\n",
    "        #Return Shape, Reference Frame and Transformation Matrix of a Raster File\n",
    "        with rio.open(raster_path) as raster:\n",
    "            return raster.shape, str(raster.crs), tuple(raster.transform)\n",
    "    \n",
    "    def _complete_catalog(self, df:pd.DataFrame) -> None:\n",
    "        \"\"\"Add metadata columns to catalog and assign self.downloaded_dataset\"\"\"\n",
    "\n",
    "        metadata = df.name.apply(lambda x: self._get_raster_metadata((self.mask_dir/x)))\n",
    "        df[[\"shape\", \"crs\", \"transform\"]] = pd.DataFrame(metadata.tolist(), index = df.index) \n",
    "        #df[[\"shape\", \"crs\", \"transform\"]] = metadata.apply(pd.Series) \n",
    "        df[\"image_path\"] = df.name.apply(lambda x: self.image_dir / x)\n",
    "        df[\"mask_path\"] = df.name.apply(lambda x: self.mask_dir / x)\n",
    "        self.downloaded_dataset = df.copy(deep = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InriaETL(DatasetETL):\n",
    "    def __init__(self, root:Path, low_storage_mode:bool = True):\n",
    "\n",
    "        self.locations = [\"austin\", \"chicago\", \"kitsap\", \"tyrol-w\", \"vienna\"]\n",
    "        self.files_list = [f\"{location}{num}.tif\" for location in self.locations for num in range(1, 37)]\n",
    "        dataset_dir = root / \"AerialImageDataset\" / \"train\"\n",
    "\n",
    "        super().__init__(\n",
    "            root = root, \n",
    "            urls = self._get_source_urls([\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.001\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.002\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.003\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.004\",\n",
    "                    \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.005\"\n",
    "                                        ]),\n",
    "            image_dir = dataset_dir / \"images\",\n",
    "            mask_dir = dataset_dir / \"gt\",\n",
    "            low_storage_mode = low_storage_mode\n",
    "        )\n",
    "\n",
    "    def download(self):\n",
    "        self._download_source_urls(self.download_dir)\n",
    "    \n",
    "    def extract(self):\n",
    "        #Verify Download\n",
    "        dataset_zip_path = self.download_dir / \"NEW2-AerialImageDataset.zip\" \n",
    "        merged_7zip_path = self.download_dir / \"aerialimagelabeling-merged.7z\" \n",
    "\n",
    "        #if self._validate_download_files(downloaded_7zip_files_list):\n",
    "            #if merged_7zip_path.exists():\n",
    "                #if dataset_zip_path.exists():\n",
    "                    ##Extract Train Folder \n",
    "                #else:\n",
    "                    ##Extract Dataset Zip\n",
    "                    ##Extract Train Folder\n",
    "            #else:\n",
    "                #if dataset_zip_path.exists():\n",
    "                    ##Extract Train Folder\n",
    "                #else:\n",
    "                    ##Extract Merge Folder\n",
    "                    ##Extract Dataset Zip\n",
    "                    ##Extract Train Folder\n",
    "\n",
    "        missing_volumes = self._validate_download()\n",
    "        if missing_volumes:\n",
    "            print(\"Missing Volumes\")\n",
    "            print(\"Please Download Missing Volumes\")\n",
    "            print(missing_volumes)\n",
    "            return\n",
    "        else:\n",
    "            print(\"Found All Volumes\")\n",
    "\n",
    "        #Merge Volumes To One Archive\n",
    "        print(f\"Merging volumes to {merged_7zip_path}\")\n",
    "        downloaded_volume_paths = self.download_dir.glob(\"*.7z.*\") \n",
    "        self._merge_multivolume_archive(downloaded_volume_paths, merged_7zip_path)\n",
    "\n",
    "        #Delete downloaded volumes        \n",
    "        if self.low_storage_mode:\n",
    "            print(\"Deleting downloaded volumes to save storage space\")\n",
    "            self._delete_files(downloaded_volume_paths)\n",
    "        \n",
    "        print(f\"Decompressing {merged_7zip_path}\")\n",
    "        self._extract_7zip(merged_7zip_path, self.download_dir)\n",
    "        print(f\"Decompressed to {self.download_dir}\")\n",
    "\n",
    "        print(\"Deleting merged archive to save storage space\")\n",
    "        self._delete_files([merged_7zip_path])\n",
    "\n",
    "        print(f\"Extracting Dataset Folder from {dataset_zip_path}\")\n",
    "        self._extract_zip(dataset_zip_path, self.root_dir)\n",
    "\n",
    "        missing_images, missing_masks = self._validate_extraction()\n",
    "        if missing_images or missing_masks:\n",
    "            print(f\"Images Not Found: {missing_images}\")\n",
    "            print(f\"Masks Not Found: {missing_masks}\")\n",
    "        else:\n",
    "            print(\"Extraction Complete\")\n",
    "            print(\"Deleting dataset archive\")\n",
    "            self._delete_files([dataset_zip_path])\n",
    "\n",
    "    #TODO: Implement Verify Extraction\n",
    "    def catalog(self):\n",
    "        df = pd.DataFrame({\"name\": self.files_list,\n",
    "                           \"split\": list(map(self._get_split, self.files_list))})\n",
    "        self._complete_catalog(df)\n",
    "    \n",
    "    def _validate_extraction(self) -> tuple:\n",
    "        return (self._validate_files(self.image_dir, self.files_list),\n",
    "                self._validate_files(self.mask_dir, self.files_list))\n",
    "\n",
    "    def _get_split(self, file_name:str):\n",
    "        \"\"\"First 6 (16.67%) in every region for testing \"\"\"\n",
    "        numbers = [char for char in file_name if char.isdigit()]\n",
    "        if int(''.join(numbers)) <= 6:\n",
    "            return \"test\"\n",
    "        return \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria = InriaETL(DATA / \"inria\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massachussets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassachussetsETL(DatasetETL):\n",
    "    mass_urls = {\n",
    "        \"train\": (\"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/train/sat/index.html\", \"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/train/map/index.html\"),\n",
    "        \"test\" : (\"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/test/sat/index.html\", \"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/test/map/index.html\"),\n",
    "        \"val\": (\"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/valid/sat/index.html\", \"https://www.cs.toronto.edu/~vmnih/data/mass_buildings/valid/map/index.html\"),\n",
    "    }\n",
    "    def __init__(self, root:Path):\n",
    "        super().__init__(root, self._get_source_urls())\n",
    "        self.image_dir = self.root_dir / \"images\"\n",
    "        self.image_dir.mkdir(exist_ok=True)\n",
    "        self.mask_dir = self.root_dir / \"masks\"\n",
    "        self.mask_dir.mkdir(exist_ok=True)\n",
    "        self.files_list = [url.split(\"/\")[-1] for url in self.source_urls[0]]\n",
    "\n",
    "    def _get_file_urls(self, url) -> list:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            pattern = r'<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"'\n",
    "            matches = re.findall(pattern, response.text)\n",
    "            return matches\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code)\n",
    "            return []\n",
    "\n",
    "    def _get_source_urls(self) -> tuple:\n",
    "        images = sum([self._get_file_urls(self.mass_urls[key][0]) for key in self.mass_urls.keys()], [])\n",
    "        masks = sum([self._get_file_urls(self.mass_urls[key][1]) for key in self.mass_urls.keys()], [])\n",
    "        return images, masks\n",
    "\n",
    "    def download(self, urls:list, target_dir_path:Path):\n",
    "        for url in urls:\n",
    "            downloaded_file_path = target_dir_path / url.split(\"/\")[-1]\n",
    "            if downloaded_file_path.exists():\n",
    "                print(f\"{url.split('/')[-1]} Exists, Skipping\")\n",
    "                continue\n",
    "            self.download_file(url, downloaded_file_path)\n",
    "    \n",
    "    def download_images(self) -> None:\n",
    "        self.download(self.source_urls[0], self.image_dir)\n",
    "        \n",
    "    def download_masks(self) -> None:\n",
    "        self.download(self.source_urls[1], self.image_dir)\n",
    "        \n",
    "    def _validate_download_dir(self, target_dir_path:Path) -> list:\n",
    "        files_not_downloaded = list()\n",
    "        for file_name in self.files_list:\n",
    "            downloaded_file_path = target_dir_path / file_name\n",
    "            if not downloaded_file_path.exists():\n",
    "                files_not_downloaded.append(downloaded_file_path)\n",
    "        return files_not_downloaded\n",
    "    \n",
    "    def validate_images(self) -> None:\n",
    "        return self._validate_download_dir(self.image_dir)\n",
    "\n",
    "    def validate_masks(self) -> None:\n",
    "        return self._validate_download_dir(self.mask_dir)\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        metadata = [self.get_raster_metadata((self.mask_dir / x)) for x in self.files_list] \n",
    "        shapes, crses, transforms = zip(*metadata)\n",
    "        data = {\n",
    "            \"name\": self.files_list,\n",
    "            \"shape\": shapes, \n",
    "            \"crs\": crses,\n",
    "            \"transform\": transforms,\n",
    "            \"image_path\": [(self.image_dir / x) for x in self.files_list],\n",
    "            \"mask_path\": [(self.mask_dir / x) for x in self.files_list]\n",
    "        }\n",
    "        return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "massachussets = MassachussetsETL(ROOT / \"massachussets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22678915_15.tiff Exists, Skipping\n",
      "22678930_15.tiff Exists, Skipping\n",
      "22678945_15.tiff Exists, Skipping\n",
      "22678960_15.tiff Exists, Skipping\n",
      "22678975_15.tiff Exists, Skipping\n",
      "22678990_15.tiff Exists, Skipping\n",
      "22679005_15.tiff Exists, Skipping\n",
      "22679020_15.tiff Exists, Skipping\n",
      "22679035_15.tiff Exists, Skipping\n",
      "22679050_15.tiff Exists, Skipping\n",
      "22828915_15.tiff Exists, Skipping\n",
      "22828945_15.tiff Exists, Skipping\n",
      "22828960_15.tiff Exists, Skipping\n",
      "22828975_15.tiff Exists, Skipping\n",
      "22829005_15.tiff Exists, Skipping\n",
      "22829020_15.tiff Exists, Skipping\n",
      "22829035_15.tiff Exists, Skipping\n",
      "22978870_15.tiff Exists, Skipping\n",
      "22978885_15.tiff Exists, Skipping\n",
      "22978900_15.tiff Exists, Skipping\n",
      "22978915_15.tiff Exists, Skipping\n",
      "22978930_15.tiff Exists, Skipping\n",
      "22978960_15.tiff Exists, Skipping\n",
      "22978975_15.tiff Exists, Skipping\n",
      "22978990_15.tiff Exists, Skipping\n",
      "22979005_15.tiff Exists, Skipping\n",
      "22979020_15.tiff Exists, Skipping\n",
      "22979035_15.tiff Exists, Skipping\n",
      "22979050_15.tiff Exists, Skipping\n",
      "22979065_15.tiff Exists, Skipping\n",
      "23128870_15.tiff Exists, Skipping\n",
      "23128885_15.tiff Exists, Skipping\n",
      "23128900_15.tiff Exists, Skipping\n",
      "23128915_15.tiff Exists, Skipping\n",
      "23128930_15.tiff Exists, Skipping\n",
      "23128945_15.tiff Exists, Skipping\n",
      "23128960_15.tiff Exists, Skipping\n",
      "23128975_15.tiff Exists, Skipping\n",
      "23128990_15.tiff Exists, Skipping\n",
      "23129005_15.tiff Exists, Skipping\n",
      "23129020_15.tiff Exists, Skipping\n",
      "23129035_15.tiff Exists, Skipping\n",
      "23129050_15.tiff Exists, Skipping\n",
      "23129065_15.tiff Exists, Skipping\n",
      "23129125_15.tiff Exists, Skipping\n",
      "23129140_15.tiff Exists, Skipping\n",
      "23129155_15.tiff Exists, Skipping\n",
      "23129170_15.tiff Exists, Skipping\n",
      "23278885_15.tiff Exists, Skipping\n",
      "23278900_15.tiff Exists, Skipping\n",
      "23278915_15.tiff Exists, Skipping\n",
      "23278930_15.tiff Exists, Skipping\n",
      "23278945_15.tiff Exists, Skipping\n",
      "23278960_15.tiff Exists, Skipping\n",
      "23278975_15.tiff Exists, Skipping\n",
      "23278990_15.tiff Exists, Skipping\n",
      "23279005_15.tiff Exists, Skipping\n",
      "23279020_15.tiff Exists, Skipping\n",
      "23279035_15.tiff Exists, Skipping\n",
      "23279050_15.tiff Exists, Skipping\n",
      "23279080_15.tiff Exists, Skipping\n",
      "23279095_15.tiff Exists, Skipping\n",
      "23279140_15.tiff Exists, Skipping\n",
      "23279155_15.tiff Exists, Skipping\n",
      "23279170_15.tiff Exists, Skipping\n",
      "23428900_15.tiff Exists, Skipping\n",
      "23428915_15.tiff Exists, Skipping\n",
      "23428930_15.tiff Exists, Skipping\n",
      "23428945_15.tiff Exists, Skipping\n",
      "23428960_15.tiff Exists, Skipping\n",
      "23428975_15.tiff Exists, Skipping\n",
      "23428990_15.tiff Exists, Skipping\n",
      "23429005_15.tiff Exists, Skipping\n",
      "23429035_15.tiff Exists, Skipping\n",
      "23429050_15.tiff Exists, Skipping\n",
      "23429065_15.tiff Exists, Skipping\n",
      "23429095_15.tiff Exists, Skipping\n",
      "23429125_15.tiff Exists, Skipping\n",
      "23429140_15.tiff Exists, Skipping\n",
      "23429170_15.tiff Exists, Skipping\n",
      "23578915_15.tiff Exists, Skipping\n",
      "23578930_15.tiff Exists, Skipping\n",
      "23578945_15.tiff Exists, Skipping\n",
      "23578975_15.tiff Exists, Skipping\n",
      "23578990_15.tiff Exists, Skipping\n",
      "23579020_15.tiff Exists, Skipping\n",
      "23579035_15.tiff Exists, Skipping\n",
      "23579065_15.tiff Exists, Skipping\n",
      "23579080_15.tiff Exists, Skipping\n",
      "23579095_15.tiff Exists, Skipping\n",
      "23579110_15.tiff Exists, Skipping\n",
      "23579125_15.tiff Exists, Skipping\n",
      "23579140_15.tiff Exists, Skipping\n",
      "23728840_15.tiff Exists, Skipping\n",
      "23728945_15.tiff Exists, Skipping\n",
      "23728960_15.tiff Exists, Skipping\n",
      "23728975_15.tiff Exists, Skipping\n",
      "23728990_15.tiff Exists, Skipping\n",
      "23729005_15.tiff Exists, Skipping\n",
      "23729020_15.tiff Exists, Skipping\n",
      "23729050_15.tiff Exists, Skipping\n",
      "23729065_15.tiff Exists, Skipping\n",
      "23729080_15.tiff Exists, Skipping\n",
      "23729095_15.tiff Exists, Skipping\n",
      "23729110_15.tiff Exists, Skipping\n",
      "23878915_15.tiff Exists, Skipping\n",
      "23878930_15.tiff Exists, Skipping\n",
      "23878945_15.tiff Exists, Skipping\n",
      "23878975_15.tiff Exists, Skipping\n",
      "23878990_15.tiff Exists, Skipping\n",
      "23879020_15.tiff Exists, Skipping\n",
      "23879035_15.tiff Exists, Skipping\n",
      "23879050_15.tiff Exists, Skipping\n",
      "23879065_15.tiff Exists, Skipping\n",
      "23879095_15.tiff Exists, Skipping\n",
      "23879110_15.tiff Exists, Skipping\n",
      "24029035_15.tiff Exists, Skipping\n",
      "24029050_15.tiff Exists, Skipping\n",
      "24029065_15.tiff Exists, Skipping\n",
      "24029080_15.tiff Exists, Skipping\n",
      "24029110_15.tiff Exists, Skipping\n",
      "24179020_15.tiff Exists, Skipping\n",
      "24179035_15.tiff Exists, Skipping\n",
      "24179050_15.tiff Exists, Skipping\n",
      "24179080_15.tiff Exists, Skipping\n",
      "24328840_15.tiff Exists, Skipping\n",
      "24328855_15.tiff Exists, Skipping\n",
      "24328870_15.tiff Exists, Skipping\n",
      "24329020_15.tiff Exists, Skipping\n",
      "24329035_15.tiff Exists, Skipping\n",
      "24329095_15.tiff Exists, Skipping\n",
      "24478840_15.tiff Exists, Skipping\n",
      "24478855_15.tiff Exists, Skipping\n",
      "24478870_15.tiff Exists, Skipping\n",
      "24478885_15.tiff Exists, Skipping\n",
      "24478900_15.tiff Exists, Skipping\n",
      "24479005_15.tiff Exists, Skipping\n",
      "22828930_15.tiff Exists, Skipping\n",
      "22828990_15.tiff Exists, Skipping\n",
      "22829050_15.tiff Exists, Skipping\n",
      "23429020_15.tiff Exists, Skipping\n",
      "23429080_15.tiff Exists, Skipping\n",
      "23578960_15.tiff Exists, Skipping\n",
      "23579005_15.tiff Exists, Skipping\n",
      "23729035_15.tiff Exists, Skipping\n",
      "23879080_15.tiff Exists, Skipping\n",
      "24179065_15.tiff Exists, Skipping\n",
      "22978945_15.tiff Exists, Skipping\n",
      "23429155_15.tiff Exists, Skipping\n",
      "23579050_15.tiff Exists, Skipping\n",
      "23728930_15.tiff Exists, Skipping\n",
      "22678915_15.tif Exists, Skipping\n",
      "22678930_15.tif Exists, Skipping\n",
      "22678945_15.tif Exists, Skipping\n",
      "22678960_15.tif Exists, Skipping\n",
      "22678975_15.tif Exists, Skipping\n",
      "22678990_15.tif Exists, Skipping\n",
      "22679005_15.tif Exists, Skipping\n",
      "22679020_15.tif Exists, Skipping\n",
      "22679035_15.tif Exists, Skipping\n",
      "22679050_15.tif Exists, Skipping\n",
      "22828915_15.tif Exists, Skipping\n",
      "22828945_15.tif Exists, Skipping\n",
      "22828960_15.tif Exists, Skipping\n",
      "22828975_15.tif Exists, Skipping\n",
      "22829005_15.tif Exists, Skipping\n",
      "22829020_15.tif Exists, Skipping\n",
      "22829035_15.tif Exists, Skipping\n",
      "22978870_15.tif Exists, Skipping\n",
      "22978885_15.tif Exists, Skipping\n",
      "22978900_15.tif Exists, Skipping\n",
      "22978915_15.tif Exists, Skipping\n",
      "22978930_15.tif Exists, Skipping\n",
      "22978960_15.tif Exists, Skipping\n",
      "22978975_15.tif Exists, Skipping\n",
      "22978990_15.tif Exists, Skipping\n",
      "22979005_15.tif Exists, Skipping\n",
      "22979020_15.tif Exists, Skipping\n",
      "22979035_15.tif Exists, Skipping\n",
      "22979050_15.tif Exists, Skipping\n",
      "22979065_15.tif Exists, Skipping\n",
      "23128870_15.tif Exists, Skipping\n",
      "23128885_15.tif Exists, Skipping\n",
      "23128900_15.tif Exists, Skipping\n",
      "23128915_15.tif Exists, Skipping\n",
      "23128930_15.tif Exists, Skipping\n",
      "23128945_15.tif Exists, Skipping\n",
      "23128960_15.tif Exists, Skipping\n",
      "23128975_15.tif Exists, Skipping\n",
      "23128990_15.tif Exists, Skipping\n",
      "23129005_15.tif Exists, Skipping\n",
      "23129020_15.tif Exists, Skipping\n",
      "23129035_15.tif Exists, Skipping\n",
      "23129050_15.tif Exists, Skipping\n",
      "23129065_15.tif Exists, Skipping\n",
      "23129125_15.tif Exists, Skipping\n",
      "23129140_15.tif Exists, Skipping\n",
      "23129155_15.tif Exists, Skipping\n",
      "23129170_15.tif Exists, Skipping\n",
      "23278885_15.tif Exists, Skipping\n",
      "23278900_15.tif Exists, Skipping\n",
      "23278915_15.tif Exists, Skipping\n",
      "23278930_15.tif Exists, Skipping\n",
      "23278945_15.tif Exists, Skipping\n",
      "23278960_15.tif Exists, Skipping\n",
      "23278975_15.tif Exists, Skipping\n",
      "23278990_15.tif Exists, Skipping\n",
      "23279005_15.tif Exists, Skipping\n",
      "23279020_15.tif Exists, Skipping\n",
      "23279035_15.tif Exists, Skipping\n",
      "23279050_15.tif Exists, Skipping\n",
      "23279080_15.tif Exists, Skipping\n",
      "23279095_15.tif Exists, Skipping\n",
      "23279140_15.tif Exists, Skipping\n",
      "23279155_15.tif Exists, Skipping\n",
      "23279170_15.tif Exists, Skipping\n",
      "23428900_15.tif Exists, Skipping\n",
      "23428915_15.tif Exists, Skipping\n",
      "23428930_15.tif Exists, Skipping\n",
      "23428945_15.tif Exists, Skipping\n",
      "23428960_15.tif Exists, Skipping\n",
      "23428975_15.tif Exists, Skipping\n",
      "23428990_15.tif Exists, Skipping\n",
      "23429005_15.tif Exists, Skipping\n",
      "23429035_15.tif Exists, Skipping\n",
      "23429050_15.tif Exists, Skipping\n",
      "23429065_15.tif Exists, Skipping\n",
      "23429095_15.tif Exists, Skipping\n",
      "23429125_15.tif Exists, Skipping\n",
      "23429140_15.tif Exists, Skipping\n",
      "23429170_15.tif Exists, Skipping\n",
      "23578915_15.tif Exists, Skipping\n",
      "23578930_15.tif Exists, Skipping\n",
      "23578945_15.tif Exists, Skipping\n",
      "23578975_15.tif Exists, Skipping\n",
      "23578990_15.tif Exists, Skipping\n",
      "23579020_15.tif Exists, Skipping\n",
      "23579035_15.tif Exists, Skipping\n",
      "23579065_15.tif Exists, Skipping\n",
      "23579080_15.tif Exists, Skipping\n",
      "23579095_15.tif Exists, Skipping\n",
      "23579110_15.tif Exists, Skipping\n",
      "23579125_15.tif Exists, Skipping\n",
      "23579140_15.tif Exists, Skipping\n",
      "23728840_15.tif Exists, Skipping\n",
      "23728945_15.tif Exists, Skipping\n",
      "23728960_15.tif Exists, Skipping\n",
      "23728975_15.tif Exists, Skipping\n",
      "23728990_15.tif Exists, Skipping\n",
      "23729005_15.tif Exists, Skipping\n",
      "23729020_15.tif Exists, Skipping\n",
      "23729050_15.tif Exists, Skipping\n",
      "23729065_15.tif Exists, Skipping\n",
      "23729080_15.tif Exists, Skipping\n",
      "23729095_15.tif Exists, Skipping\n",
      "23729110_15.tif Exists, Skipping\n",
      "23878915_15.tif Exists, Skipping\n",
      "23878930_15.tif Exists, Skipping\n",
      "23878945_15.tif Exists, Skipping\n",
      "23878975_15.tif Exists, Skipping\n",
      "23878990_15.tif Exists, Skipping\n",
      "23879020_15.tif Exists, Skipping\n",
      "23879035_15.tif Exists, Skipping\n",
      "23879050_15.tif Exists, Skipping\n",
      "23879065_15.tif Exists, Skipping\n",
      "23879095_15.tif Exists, Skipping\n",
      "23879110_15.tif Exists, Skipping\n",
      "24029035_15.tif Exists, Skipping\n",
      "24029050_15.tif Exists, Skipping\n",
      "24029065_15.tif Exists, Skipping\n",
      "24029080_15.tif Exists, Skipping\n",
      "24029110_15.tif Exists, Skipping\n",
      "24179020_15.tif Exists, Skipping\n",
      "24179035_15.tif Exists, Skipping\n",
      "24179050_15.tif Exists, Skipping\n",
      "24179080_15.tif Exists, Skipping\n",
      "24328840_15.tif Exists, Skipping\n",
      "24328855_15.tif Exists, Skipping\n",
      "24328870_15.tif Exists, Skipping\n",
      "24329020_15.tif Exists, Skipping\n",
      "24329035_15.tif Exists, Skipping\n",
      "24329095_15.tif Exists, Skipping\n",
      "24478840_15.tif Exists, Skipping\n",
      "24478855_15.tif Exists, Skipping\n",
      "24478870_15.tif Exists, Skipping\n",
      "24478885_15.tif Exists, Skipping\n",
      "24478900_15.tif Exists, Skipping\n",
      "24479005_15.tif Exists, Skipping\n",
      "22828930_15.tif Exists, Skipping\n",
      "22828990_15.tif Exists, Skipping\n",
      "22829050_15.tif Exists, Skipping\n",
      "23429020_15.tif Exists, Skipping\n",
      "23429080_15.tif Exists, Skipping\n",
      "23578960_15.tif Exists, Skipping\n",
      "23579005_15.tif Exists, Skipping\n",
      "23729035_15.tif Exists, Skipping\n",
      "23879080_15.tif Exists, Skipping\n",
      "24179065_15.tif Exists, Skipping\n",
      "22978945_15.tif Exists, Skipping\n",
      "23429155_15.tif Exists, Skipping\n",
      "23579050_15.tif Exists, Skipping\n",
      "23728930_15.tif Exists, Skipping\n"
     ]
    }
   ],
   "source": [
    "massachussets.download_images()\n",
    "massachussets.download_masks()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISPRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISPRSSemanticLabelingETL(DatasetETL):\n",
    "    isprs_urls = {\n",
    "        \"potsdam.zip\": \"https://seafile.projekt.uni-hannover.de/f/429be50cc79d423ab6c4/\",\n",
    "        \"toronto.zip\": \"https://seafile.projekt.uni-hannover.de/f/fc62f9c20a8c4a34aea1/\",\n",
    "        \"vaihingen.zip\": \"https://seafile.projekt.uni-hannover.de/f/6a06a837b1f349cfa749/\",\n",
    "    }\n",
    "    password = \"CjwcipT4-P8g\"\n",
    "    cookie_name = \"sfcsrftoken\"\n",
    "\n",
    "    def __init__(self, root:Path, low_storage_mode:bool = True):\n",
    "        super().__init__(root = root, \n",
    "                         source_urls = self.isprs_urls, \n",
    "                         low_storage_mode = low_storage_mode\n",
    "        )\n",
    "\n",
    "    def _download_file(self, file_path:str, url:str, chunk_size = 1024*1024) -> None:\n",
    "        session = requests.Session()\n",
    "        cookies = {self.cookie_name: session.get(url).cookies.get(self.cookie_name)}\n",
    "        payload:dict = {'csrfmiddlewaretoken': cookies[self.cookie_name], \n",
    "                        'password': self.password}\n",
    "        with requests.post(url+\"?dl=1\", data = payload, cookies = cookies, stream = True) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            with open(file_path, \"wb\") as f, tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk: \n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "    \n",
    "    def download(self):\n",
    "        self._download_source_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "isprs = ISPRSSemanticLabelingETL(ROOT / \"isprs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   9%|▊         | 1.16G/13.3G [16:28<2:52:33, 1.17MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m isprs\u001b[39m.\u001b[39;49mdownload()\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mISPRSSemanticLabelingETL.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m location, url \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misprs_urls\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     28\u001b[0m     zip_file_path:Path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlocation\u001b[39m}\u001b[39;00m\u001b[39m.zip\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_file(zip_file_path, url)\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mISPRSSemanticLabelingETL.download_file\u001b[1;34m(self, file_path, url, chunk_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m total_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m     20\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f, tqdm(total\u001b[39m=\u001b[39mtotal_size, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m progress_bar:\n\u001b[1;32m---> 21\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mchunk_size):\n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m chunk: \n\u001b[0;32m     23\u001b[0m             f\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\ml\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "isprs.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityOSMETL(DatasetETL):\n",
    "    city_osm_urls = {\n",
    "        \"Berlin.zip\": \"https://zenodo.org/record/1154821/files/berlin.zip?download=1\",\n",
    "        \"Chicago.zip\": \"https://zenodo.org/record/1154821/files/chicago.zip?download=1\",\n",
    "        \"Paris.zip\": \"https://zenodo.org/record/1154821/files/paris.zip?download=1\",\n",
    "        \"Potsdam.zip\": \"https://zenodo.org/record/1154821/files/potsdam.zip?download=1\",\n",
    "        \"Tokyo.zip\": \"https://zenodo.org/record/1154821/files/tokyo.zip?download=1\",\n",
    "        \"Zurich.zip\": \"https://zenodo.org/record/1154821/files/zurich.zip?download=1\"\n",
    "    }\n",
    "    def __init__(self, root:Path):\n",
    "        super().__init__(root, self.city_osm_urls)\n",
    "    \n",
    "    def download(self):\n",
    "        self._download_source_urls()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_osm = CityOSMETL(DATA / \"city-osm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.00G/2.00G [09:44<00:00, 3.43MB/s] \n",
      "Downloading: 100%|██████████| 5.42G/5.42G [42:36<00:00, 2.12MB/s]  \n",
      "Downloading:  16%|█▋        | 1.80G/11.0G [37:04<5:13:55, 486kB/s] "
     ]
    }
   ],
   "source": [
    "city_osm.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceNetETL(DatasetETL):\n",
    "    \n",
    "    def __init__(self, root:Path):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
