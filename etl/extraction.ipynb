{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "\n",
    "#import nest_asyncio \n",
    "#nest_asyncio.apply()\n",
    "\n",
    "import zipfile\n",
    "import py7zr\n",
    "import multivolumefile\n",
    "\n",
    "import uuid\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"/media/sambhav/30AC4696AC46568E/datasets/urban-feature-extraction\")\n",
    "#DATA = Path(\"C:/Users/hp/Desktop/datasets/urban-feature-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDownloader:\n",
    "    def __init__(self, downloads:Path, urls:dict):\n",
    "\n",
    "        # init required directories\n",
    "        if not (downloads.exists() and downloads.is_dir()):\n",
    "            downloads.mkdir(exist_ok=True, parents=True)\n",
    "            print(f\"download directory at {downloads}\")\n",
    "        self.download_dir = downloads\n",
    "\n",
    "        # src_urls is a dictionary such that\n",
    "        # src_urls[filename:str]  = url:str\n",
    "        self.src_urls = urls\n",
    "        \n",
    "    async def download_one_file(self, session, url:str, file_path:Path):\n",
    "        \"\"\"Download one file from url and save to disk at file_path\"\"\"\n",
    "        #TODO: How to use tqdm for each coroutine in the notebook\n",
    "        async with session.get(url, ssl = False) as r:\n",
    "            #total_size = int(r.headers.get('content-length', 0))\n",
    "            async with aiofiles.open(file_path, \"wb\") as f:\n",
    "                #progress_bar = tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\")\n",
    "                async for chunk in r.content.iter_any():\n",
    "                    await f.write(chunk)\n",
    "                    #progress_bar.update(len(chunk))\n",
    "\n",
    "    async def download_files(self) -> None:\n",
    "        #Download files from self.src_urls, skip if already_downloaded\n",
    "        timeout = aiohttp.ClientTimeout(total = None)\n",
    "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "            coroutines = list()\n",
    "            for file_name, url in self.src_urls.items():\n",
    "                file_path = self.download_dir / file_name \n",
    "                coroutines.append(self.download_one_file(session, url, file_path))\n",
    "            await tqdm_asyncio.gather(*coroutines)   \n",
    "    \n",
    "    def validate_download(self, downloaded_file_sizes: dict) -> None:\n",
    "        #TODO: Implement Validate Downloads\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetExtractor:\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_zip_archive(zip_file_path:Path, target_dir:Path, dirs_to_be_extracted = list()):\n",
    "        \"\"\"Extract specified contents from zip archive, extract all contents if not specified\"\"\"\n",
    "\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip:\n",
    "            #If dirs_to_be_extracted is an empty list, extract entire archive and exit\n",
    "            if not dirs_to_be_extracted:\n",
    "                zip.extractall(target_dir); return\n",
    "            #Otherwise, extract all files under specified dirs\n",
    "            #For each file in archive, extract if it's under any specified dir\n",
    "            for member in zip.infolist():\n",
    "                for foldername in dirs_to_be_extracted:\n",
    "                    if foldername in member.filename:\n",
    "                        #TODO: Add tqdm progress bar for extraction\n",
    "                        zip.extract(member, target_dir)\n",
    "\n",
    "    @staticmethod    \n",
    "    def extract_multivolume_archive(multivolume_file_path:Path, target_dir:Path) -> None:\n",
    "        \"\"\"Extract all contents of a multivolume 7zip archive\"\"\" \n",
    "\n",
    "        with multivolumefile.open(multivolume_file_path, mode = 'rb') as multi_archive:\n",
    "            with py7zr.SevenZipFile(multi_archive, 'r') as archive: # type: ignore\n",
    "                archive.extractall(path = target_dir)\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_extraction(val_dir:Path, val_files: list):\n",
    "        \"\"\"Check if val_dir contains all files listed under val_files, return list of missing files\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropUtil():\n",
    "\n",
    "    def _read_image(self, path):\n",
    "        return skimage.io.imread(path) \n",
    "\n",
    "    def _read_mask(self, path):\n",
    "        return skimage.io.imread(path) \n",
    "\n",
    "    def _get_pad_amount(self, dimension: int, window: int):\n",
    "        \"\"\"Calculate to no of pixels to add to before and after dimension\"\"\"\n",
    "        total_padding = window - (dimension % window)\n",
    "\n",
    "        if total_padding % 2 == 0:\n",
    "            after = total_padding // 2\n",
    "            before = after\n",
    "        else:\n",
    "            after = (total_padding // 2) + 1\n",
    "            before = after - 1\n",
    "        assert before+after == total_padding \n",
    "        return (before, after)\n",
    "    \n",
    "    def _pad_3d_array(self, array: np.ndarray, window: int):\n",
    "        \"\"\"\n",
    "        Pad image array s.t. divisible by window\\n\n",
    "        array.shape : (Height, Width, Channels)\n",
    "        window : side length of square cropping window\n",
    "        \"\"\"\n",
    "\n",
    "        assert array.ndim == 3\n",
    "        padded_array = np.pad(\n",
    "            array = array,\n",
    "            pad_width = (self._get_pad_amount(array.shape[0], window),\n",
    "                         self._get_pad_amount(array.shape[1], window),\n",
    "                         (0, 0))\n",
    "        ) \n",
    "        return padded_array\n",
    "    \n",
    "    def _get_cropped_view(self, array: np.ndarray, window:int):\n",
    "        \"\"\"\n",
    "        Crop image array s.t. divisible by window\\n\n",
    "        array.shape : (Height, Width, Channels)\n",
    "        window : side length of square cropping window\n",
    "        \"\"\"\n",
    "\n",
    "        assert array.ndim == 3\n",
    "        cropped_view = skimage.util.view_as_windows(\n",
    "            arr_in = array,\n",
    "            window_shape = (window, window, array.shape[2]),\n",
    "            step =  (window, window, array.shape[2])).squeeze()\n",
    "            \n",
    "        cropped_view = cropped_view.reshape(-1, window, window, array.shape[2])\n",
    "\n",
    "        return cropped_view\n",
    "\n",
    "    def _crop_one_scene(self, tile_path: Path, window: int, read_scene):\n",
    "        scene = read_scene(tile_path) \n",
    "        scene = self._pad_3d_array(scene, window)\n",
    "        scene = self._get_cropped_view(scene, window)\n",
    "        return scene\n",
    "\n",
    "    def _save_as_jpeg_100(self, array: np.ndarray, out_path: Path) -> None:\n",
    "        skimage.io.imsave((out_path.parent / f\"{out_path.stem}.jpg\"), array, check_contrast = False, **{\"quality\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetUploader():\n",
    "    def __init__(self, bucket_name):\n",
    "        self.init_boto3()\n",
    "        self.s3resource = boto3.resource('s3', endpoint_url = \"https://usc1.contabostorage.com\")\n",
    "        self.bucket = self.s3resource.Bucket(bucket_name)\n",
    "\n",
    "        self.prefix = {\n",
    "            \"train_image\": \"labelled/patches/images\",\n",
    "            \"train_mask\": \"labelled/patches/masks\",\n",
    "\n",
    "            \"test_image\": \"labelled/scenes/images\",\n",
    "            \"test_mask\": \"labelled/scenes/masks\",\n",
    "        }\n",
    "    \n",
    "    def init_boto3(self, aws_dir = None):\n",
    "        if not aws_dir: \n",
    "            aws_dir = (Path.home() / \".aws\")\n",
    "        #os.environ[\"AWS_CONFIG_FILE\"] = (aws_dir / \"config\").as_posix()\n",
    "        os.environ[\"AWS_SHARED_CREDENTIALS_FILE\"] = (aws_dir / \"credentials\").as_posix()\n",
    "    \n",
    "    def list_files_in_bucket(self):\n",
    "        for file_obj in self.bucket.objects.all():\n",
    "            print(file_obj.key)\n",
    "    \n",
    "    def upload_patch(self, patch_path:Path, prefix:str):\n",
    "        assert patch_path.exists() and patch_path.is_file()\n",
    "\n",
    "        patch_key = (Path(prefix) / patch_path.name).as_posix()\n",
    "        self.bucket.Object(patch_key).upload_file(patch_path)\n",
    "        print(f\"Uploaded to {self.bucket.name}/{patch_key}\")\n",
    "    \n",
    "    def upload_train_pair(self, file_name, image_dir, mask_dir):\n",
    "        self.upload_patch(\n",
    "            patch_path = (image_dir / file_name),\n",
    "            prefix = self.prefix[\"train_image\"]\n",
    "        )\n",
    "        self.upload_patch(\n",
    "            patch_path = (mask_dir / file_name),\n",
    "            prefix = self.prefix[\"train_mask\"]\n",
    "        )\n",
    "\n",
    "    def upload_test_pair(self, file_name, image_dir, mask_dir):\n",
    "        self.upload_patch(\n",
    "            patch_path = (image_dir / file_name),\n",
    "            prefix = self.prefix[\"test_image\"]\n",
    "        )\n",
    "        self.upload_patch(\n",
    "            patch_path = (mask_dir / file_name),\n",
    "            prefix = self.prefix[\"test_mask\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InriaETL:\n",
    "    urls = {\n",
    "        \"aerialimagelabeling.7z.001\" : \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.001\",\n",
    "        \"aerialimagelabeling.7z.002\" :   \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.002\",\n",
    "        \"aerialimagelabeling.7z.003\" :  \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.003\",\n",
    "        \"aerialimagelabeling.7z.004\" :  \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.004\",\n",
    "        \"aerialimagelabeling.7z.005\" :  \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.005\"\n",
    "    }\n",
    "\n",
    "    sizes = {\n",
    "        \"aerialimagelabeling.7z.001\" : 0,\n",
    "        \"aerialimagelabeling.7z.002\" : 0,\n",
    "        \"aerialimagelabeling.7z.003\" : 0,\n",
    "        \"aerialimagelabeling.7z.004\" : 0,\n",
    "        \"aerialimagelabeling.7z.005\" : 0\n",
    "    }\n",
    "\n",
    "    locations = [\"austin\", \"chicago\", \"kitsap\", \"tyrol-w\", \"vienna\"]\n",
    "    test_files_list = [f\"{location}{num}.tif\" for location in locations for num in range(1, 7)]\n",
    "    train_files_list = [f\"{location}{num}.tif\" for location in locations for num in range(7, 37)]\n",
    "\n",
    "    def __init__(self, root: Path):\n",
    "        self.root_dir = root\n",
    "        self.download_dir = root / \"downloads\"\n",
    "\n",
    "        # Downloaded Images, Masks\n",
    "        self.d_dataset_dir = root / \"AerialImageDataset\" / \"train\"\n",
    "        self.d_image_dir = self.d_dataset_dir / \"images\"\n",
    "        self.d_mask_dir = self.d_dataset_dir / \"gt\" \n",
    "\n",
    "        # Test Images, Masks\n",
    "        self.t_image_dir = self.root_dir / \"test\" / \"images\"\n",
    "        self.t_mask_dir = self.root_dir / \"test\" / \"masks\"\n",
    "\n",
    "        #Images, Masks\n",
    "        self.image_dir = self.root_dir / \"images\"\n",
    "        self.mask_dir = self.root_dir / \"masks\"\n",
    "\n",
    "        \n",
    "    async def download(self):\n",
    "        downloader = DatasetDownloader(self.download_dir, self.urls)\n",
    "        await asyncio.create_task(downloader.download_files())\n",
    "        #downloader.validate_download()\n",
    "    \n",
    "    def extract(self, low_storage_mode:bool):\n",
    "        extractor = DatasetExtractor()\n",
    "\n",
    "        #Merge and Extract Dataset Zip\n",
    "        multivolume_7zip_path = self.download_dir / \"aerialimagelabeling.7z\" \n",
    "        dataset_zip_path = self.download_dir / \"NEW2-AerialImageDataset.zip\" \n",
    "\n",
    "        extractor.extract_multivolume_archive(multivolume_7zip_path, self.download_dir)\n",
    "        print(f\"Extracted Multivolume Archive to {dataset_zip_path}\")\n",
    "\n",
    "        if low_storage_mode:\n",
    "            print(\"Deleting downloaded multi-volume to save storage space\")\n",
    "            for volume in self.download_dir.glob(\"aerialimagelabeling.7z.*\"):\n",
    "                volume.unlink()\n",
    "\n",
    "        extractor.extract_zip_archive(dataset_zip_path, self.root_dir, [\"train\"])\n",
    "        print(f\"Extracted Dataset Archive to {self.root_dir}\")\n",
    "\n",
    "        if low_storage_mode:\n",
    "            print(\"Deleting extracted dataset archive to save storage space\")\n",
    "            dataset_zip_path.unlink()\n",
    "\n",
    "        #extractor.validate_extraction(files_list, dataset_dir)\n",
    "\n",
    "    def move_test_split(self):\n",
    "        self.t_image_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.t_mask_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        for file_name in self.test_files_list:\n",
    "            shutil.move((self.d_image_dir / file_name), self.t_image_dir)\n",
    "            shutil.move((self.d_mask_dir / file_name), self.t_mask_dir)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_mask(path:Path):\n",
    "        return np.expand_dims(skimage.io.imread(path), -1)\n",
    "\n",
    "    def crop(self, window: int):\n",
    "        self.image_dir.mkdir(exist_ok=True)\n",
    "        self.mask_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        cropper = CropUtil()\n",
    "\n",
    "        for file_name in self.train_files_list:\n",
    "\n",
    "            cropped_image_view = cropper._crop_one_scene(\n",
    "                tile_path = self.d_image_dir / file_name,\n",
    "                window = window,\n",
    "                read_scene = cropper._read_image\n",
    "            )\n",
    "\n",
    "            cropped_mask_view = cropper._crop_one_scene(\n",
    "                tile_path = self.d_mask_dir / file_name,\n",
    "                window = window,\n",
    "                read_scene = self.read_mask \n",
    "            )\n",
    "    \n",
    "            for image_crop, mask_crop in zip(cropped_image_view, cropped_mask_view):\n",
    "                crop_name = str(uuid.uuid4())\n",
    "                cropper._save_as_jpeg_100(image_crop , (self.image_dir / crop_name)) \n",
    "                cropper._save_as_jpeg_100(mask_crop.squeeze(), (self.mask_dir / crop_name))\n",
    "\n",
    "    def delete_downloaded_dataset(self):\n",
    "        shutil.rmtree(self.d_dataset_dir)\n",
    "        self.d_dataset_dir.parent.rmdir()\n",
    "    \n",
    "    def upload(self, s3_bucket_name):\n",
    "        uploader = DatasetUploader(s3_bucket_name)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for file_path in self.mask_dir.glob(\"*.jpg\"):\n",
    "\n",
    "            if count >= 10:\n",
    "                break\n",
    "            count+=1\n",
    "            uploader.upload_train_pair(file_path.name, self.image_dir, self.mask_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria = InriaETL(DATA / \"inria\")\n",
    "#await inria.download()\n",
    "#inria.extract(low_storage_mode=True)\n",
    "#inria.crop(512)\n",
    "#inria.move_test_split()\n",
    "#inria.delete_downloaded_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded to building-footprints-dataset/labelled/patches/images/3fb79bc7-c7d4-45dd-8e4a-ac0f9d353ffd.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/3fb79bc7-c7d4-45dd-8e4a-ac0f9d353ffd.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/images/827e6c0e-5a90-46df-a008-1ad8796c9049.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/827e6c0e-5a90-46df-a008-1ad8796c9049.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/images/c3050722-061a-4323-b555-a8a36c8d7a26.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/c3050722-061a-4323-b555-a8a36c8d7a26.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/images/0005d947-64cf-4ea0-9647-8efd7f14a2a7.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/0005d947-64cf-4ea0-9647-8efd7f14a2a7.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/images/0009bf42-ae35-4aec-93bc-014feaf001a1.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/0009bf42-ae35-4aec-93bc-014feaf001a1.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/images/000cc45c-dc6f-4aa4-b144-4732e80a017c.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/000cc45c-dc6f-4aa4-b144-4732e80a017c.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/images/000d9574-9c9d-4c35-8c2f-a7e85731c495.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/000d9574-9c9d-4c35-8c2f-a7e85731c495.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/images/000fd5eb-718e-45be-94c0-2e07d32efb12.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/000fd5eb-718e-45be-94c0-2e07d32efb12.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/images/0010e133-2bbb-4b64-8fe5-7451cda8a64a.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/0010e133-2bbb-4b64-8fe5-7451cda8a64a.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/images/001535a2-060d-4527-914d-82275a3fbab8.jpg\n",
      "Uploaded to building-footprints-dataset/labelled/patches/masks/001535a2-060d-4527-914d-82275a3fbab8.jpg\n"
     ]
    }
   ],
   "source": [
    "inria.upload('building-footprints-dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
