{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install aiofiles\n",
    "%pip install py7zr\n",
    "%pip install imagecodecs\n",
    "%pip install boto3\n",
    "%pip install nest_asyncio\n",
    "%pip install multivolumefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "\n",
    "import zipfile\n",
    "import py7zr\n",
    "import multivolumefile\n",
    "\n",
    "import uuid\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDownloader:\n",
    "    def __init__(self, downloads:Path, urls:dict):\n",
    "\n",
    "        # init required directories\n",
    "        if not (downloads.exists() and downloads.is_dir()):\n",
    "            downloads.mkdir(exist_ok=True, parents=True)\n",
    "            print(f\"download directory at {downloads}\")\n",
    "        self.download_dir = downloads\n",
    "\n",
    "        # src_urls is a dictionary such that\n",
    "        # src_urls[filename:str]  = url:str\n",
    "        self.src_urls = urls\n",
    "        \n",
    "    async def async_download_one_file(self, session, url:str, file_path:Path):\n",
    "        \"\"\"Download one file from url and save to disk at file_path\"\"\"\n",
    "        #TODO: How to use tqdm for each coroutine in the notebook\n",
    "        async with session.get(url, ssl = False) as r:\n",
    "            #total_size = int(r.headers.get('content-length', 0))\n",
    "            async with aiofiles.open(file_path, \"wb\") as f:\n",
    "                #progress_bar = tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\")\n",
    "                async for chunk in r.content.iter_any():\n",
    "                    await f.write(chunk)\n",
    "                    #progress_bar.update(len(chunk))\n",
    "\n",
    "    async def download_files(self, async_download_one_file = None) -> None:\n",
    "        if not async_download_one_file:\n",
    "            async_download_one_file = self.async_download_one_file\n",
    "        #Download files from self.src_urls, skip if already_downloaded\n",
    "        timeout = aiohttp.ClientTimeout(total = None)\n",
    "        async with aiohttp.ClientSession(timeout=timeout, cookie_jar=aiohttp.CookieJar()) as session:\n",
    "            coroutines = list()\n",
    "            for file_name, url in self.src_urls.items():\n",
    "                file_path = self.download_dir / file_name \n",
    "                coroutines.append(async_download_one_file(session, url, file_path))\n",
    "            await tqdm_asyncio.gather(*coroutines)   \n",
    "    \n",
    "    def validate_download(self, downloaded_file_sizes: dict) -> None:\n",
    "        #TODO: Implement Validate Downloads\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetExtractor:\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_zip_archive(zip_file_path:Path, target_dir:Path, dirs_to_be_extracted = list()):\n",
    "        \"\"\"Extract specified contents from zip archive, extract all contents if not specified\"\"\"\n",
    "\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip:\n",
    "            #If dirs_to_be_extracted is an empty list, extract entire archive and exit\n",
    "            if not dirs_to_be_extracted:\n",
    "                zip.extractall(target_dir); return\n",
    "            #Otherwise, extract all files under specified dirs\n",
    "            #For each file in archive, extract if it's under any specified dir\n",
    "            for member in zip.infolist():\n",
    "                for foldername in dirs_to_be_extracted:\n",
    "                    if foldername in member.filename:\n",
    "                        #TODO: Add tqdm progress bar for extraction\n",
    "                        zip.extract(member, target_dir)\n",
    "\n",
    "    @staticmethod    \n",
    "    def extract_multivolume_archive(multivolume_file_path:Path, target_dir:Path) -> None:\n",
    "        \"\"\"Extract all contents of a multivolume 7zip archive\"\"\" \n",
    "\n",
    "        with multivolumefile.open(multivolume_file_path, mode = 'rb') as multi_archive:\n",
    "            with py7zr.SevenZipFile(multi_archive, 'r') as archive: # type: ignore\n",
    "                archive.extractall(path = target_dir)\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_extraction(val_dir:Path, val_files: list):\n",
    "        \"\"\"Check if val_dir contains all files listed under val_files, return list of missing files\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropUtil():\n",
    "\n",
    "    def _read_image(self, path):\n",
    "        return skimage.io.imread(path)  # type: ignore\n",
    "\n",
    "    def _read_mask(self, path):\n",
    "        return skimage.io.imread(path)  # type: ignore\n",
    "\n",
    "    def _get_pad_amount(self, dimension: int, window: int):\n",
    "        \"\"\"Calculate to no of pixels to add to before and after dimension\"\"\"\n",
    "        total_padding = window - (dimension % window)\n",
    "\n",
    "        if total_padding % 2 == 0:\n",
    "            after = total_padding // 2\n",
    "            before = after\n",
    "        else:\n",
    "            after = (total_padding // 2) + 1\n",
    "            before = after - 1\n",
    "        assert before+after == total_padding \n",
    "        return (before, after)\n",
    "    \n",
    "    def _pad_3d_array(self, array: np.ndarray, window: int):\n",
    "        \"\"\"\n",
    "        Pad image array s.t. divisible by window\\n\n",
    "        array.shape : (Height, Width, Channels)\n",
    "        window : side length of square cropping window\n",
    "        \"\"\"\n",
    "\n",
    "        assert array.ndim == 3\n",
    "        padded_array = np.pad(\n",
    "            array = array,\n",
    "            pad_width = (self._get_pad_amount(array.shape[0], window),\n",
    "                         self._get_pad_amount(array.shape[1], window),\n",
    "                         (0, 0))\n",
    "        ) \n",
    "        return padded_array\n",
    "    \n",
    "    def _get_cropped_view(self, array: np.ndarray, window:int):\n",
    "        \"\"\"\n",
    "        Crop image array s.t. divisible by window\\n\n",
    "        array.shape : (Height, Width, Channels)\n",
    "        window : side length of square cropping window\n",
    "        \"\"\"\n",
    "\n",
    "        assert array.ndim == 3\n",
    "        cropped_view = skimage.util.view_as_windows( # type: ignore\n",
    "            arr_in = array,\n",
    "            window_shape = (window, window, array.shape[2]),\n",
    "            step =  (window, window, array.shape[2])).squeeze()\n",
    "            \n",
    "        cropped_view = cropped_view.reshape(-1, window, window, array.shape[2])\n",
    "\n",
    "        return cropped_view\n",
    "\n",
    "    def _crop_one_scene(self, tile_path: Path, window: int, read_scene):\n",
    "        scene = read_scene(tile_path) \n",
    "        scene = self._pad_3d_array(scene, window)\n",
    "        scene = self._get_cropped_view(scene, window)\n",
    "        return scene\n",
    "\n",
    "    def _save_as_jpeg_100(self, array: np.ndarray, out_path: Path) -> None:\n",
    "        skimage.io.imsave((out_path.parent / f\"{out_path.stem}.jpg\"), array, check_contrast = False, **{\"quality\": 100}) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContaboStorage():\n",
    "    def __init__(self, credentials_file: Path, bucket_name: str):\n",
    "\n",
    "        assert (credentials_file.exists() and credentials_file.is_file())\n",
    "\n",
    "        with open(credentials_file) as credentials:\n",
    "            credentials = credentials.readlines()[1:]\n",
    "\n",
    "        def get_key(x: str) ->str:\n",
    "            return x.split('=')[-1].strip('\\n').strip(' ')\n",
    "\n",
    "        self.s3resource = boto3.resource(\"s3\",\n",
    "                          endpoint_url = \"https://usc1.contabostorage.com\",\n",
    "                          aws_access_key_id = get_key(credentials[0]),\n",
    "                          aws_secret_access_key = get_key(credentials[1]))\n",
    "\n",
    "        self.bucket = self.s3resource.Bucket(bucket_name) # type: ignore\n",
    "        self.prefix = {\n",
    "            \"train_image\": Path(\"labelled/patches/images\"),\n",
    "            \"train_mask\": Path(\"labelled/patches/masks\"),\n",
    "            \"train_catalog\": Path(\"labelled/metadata/\"),\n",
    "\n",
    "            \"test_image\": Path(\"labelled/scenes/images\"),\n",
    "            \"test_mask\": Path(\"labelled/scenes/masks\"),\n",
    "            \"test_catalog\": Path(\"labelled/metadata/\"),\n",
    "        }\n",
    "\n",
    "    \n",
    "    def upload_file(self, file_path:Path, bucket_key:Path):\n",
    "        assert file_path.exists() and file_path.is_file()\n",
    "        bucket_obj = self.bucket.Object(bucket_key.as_posix())\n",
    "        bucket_obj.upload_file(file_path)\n",
    "\n",
    "    def download_file(self, bucket_key:Path, file_path:Path):\n",
    "        bucket_obj = self.bucket.Object(bucket_key.as_posix())\n",
    "        file_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        bucket_obj.download_file(file_path)\n",
    "    \n",
    "    def delete_file(self, bucket_key:Path):\n",
    "        #TODO: Finish This\n",
    "        bucket_obj = self.bucket.Object(bucket_key.as_posix())\n",
    "        #bucket_obj.delete_file(missing_ok = True)\n",
    "    \n",
    "    def download_train_catalog(self, csv_path: Path):\n",
    "        self.download_file(self.prefix[\"train_catalog\"] / \"patches.csv\", csv_path)\n",
    "\n",
    "    def upload_train_catalog(self, csv_path: Path):\n",
    "        self.upload_file(csv_path, self.prefix[\"train_catalog\"] / \"patches.csv\")\n",
    "\n",
    "    def download_test_catalog(self, csv_path: Path):\n",
    "        self.download_file(self.prefix[\"test_catalog\"] / \"scenes.csv\", csv_path)\n",
    "\n",
    "    def upload_test_catalog(self, csv_path: Path):\n",
    "        self.upload_file(csv_path, self.prefix[\"test_catalog\"] / \"scenes.csv\")\n",
    "\n",
    "    def upload_train_pair(self, file_name, image_dir, mask_dir):\n",
    "        self.upload_file(file_path = (image_dir / file_name),\n",
    "                         bucket_key = self.prefix[\"train_image\"] / file_name)\n",
    "        self.upload_file(file_path = (mask_dir / file_name),\n",
    "                         bucket_key = self.prefix[\"train_mask\"] / file_name)\n",
    "\n",
    "    def upload_test_pair(self, file_name, image_dir, mask_dir):\n",
    "        self.upload_file(file_path = (image_dir / file_name),\n",
    "                         bucket_key = self.prefix[\"test_image\"] / file_name)\n",
    "        self.upload_file(file_path = (mask_dir / file_name),\n",
    "                         bucket_key = self.prefix[\"test_mask\"] / file_name)\n",
    "   \n",
    "    def delete_train_pair(self, file_name):\n",
    "        pass\n",
    "    \n",
    "    def delete_test_pair(self, file_name):\n",
    "        pass\n",
    "\n",
    "    def list_files_in_bucket(self):\n",
    "        for file_obj in self.bucket.objects.all():\n",
    "            print(file_obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3StorageEmulator():\n",
    "    def __init__(self, root: Path, credentials_file: Path, bucket_name: str):\n",
    "        #specific to emulator\n",
    "        root.mkdir(exist_ok = True, parents=True)\n",
    "\n",
    "        assert (credentials_file.exists() and credentials_file.is_file())\n",
    "\n",
    "        self.root = root \n",
    "        self.bucket = self.root / bucket_name\n",
    "\n",
    "        self.prefix = {\n",
    "            \"train_image\": Path(\"labelled/patches/images\"),\n",
    "            \"train_mask\": Path(\"labelled/patches/masks\"),\n",
    "            \"train_catalog\": Path(\"labelled/metadata/patches.csv\"),\n",
    "\n",
    "            \"test_image\": Path(\"labelled/scenes/images\"),\n",
    "            \"test_mask\": Path(\"labelled/scenes/masks\"),\n",
    "            \"test_catalog\": Path(\"labelled/metadata/scenes.csv\"),\n",
    "        }\n",
    "\n",
    "    def upload_file(self, file_path:Path, bucket_key:Path):\n",
    "        \"\"\"Upload from local storage to configured bucket\"\"\"\n",
    "\n",
    "        assert file_path.exists() and file_path.is_file(), \"File not found on disk\"\n",
    "        storage_key = self.bucket / bucket_key\n",
    "\n",
    "        #Only for Emulator, Make Sure Parent Directory Exists for Copying\n",
    "        (storage_key.parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #Emulator version of upload\n",
    "        shutil.copy(file_path, storage_key)\n",
    "\n",
    "    def download_file(self, bucket_key:Path, file_path: Path):\n",
    "        \"\"\"Download from configured bucket to local path in argument\"\"\"\n",
    "\n",
    "        #Emulate error when file does not exist on bucket\n",
    "        storage_key = self.bucket / bucket_key\n",
    "        assert storage_key.exists() and storage_key.is_file(), \"Key not found in bucket\"\n",
    "            \n",
    "        #Ensure directory in which file_path is to be downloaded exists\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #Emulator version of upload\n",
    "        shutil.copy(storage_key, file_path)\n",
    "\n",
    "    def delete_file(self, bucket_key):\n",
    "        \"\"\"Delete file from configured bucket\"\"\"\n",
    "        storage_key = self.bucket / bucket_key\n",
    "        assert storage_key.exists() and storage_key.is_file(), \"Key not found in bucket\"\n",
    "        storage_key.unlink()\n",
    "\n",
    "    def download_train_catalog(self, csv_path: Path):\n",
    "        try:\n",
    "            self.download_file(bucket_key = self.prefix[\"train_catalog\"], \n",
    "                               file_path = csv_path)\n",
    "        except AssertionError:\n",
    "            print(\"Training Catalog Not Found\")\n",
    "            return\n",
    "    \n",
    "    def upload_train_catalog(self, csv_path: Path):\n",
    "        self.upload_file(csv_path, self.prefix[\"train_catalog\"])\n",
    "\n",
    "    def upload_train_pair(self, file_name, image_dir, mask_dir):\n",
    "        self.upload_file(file_path = (image_dir / file_name),\n",
    "                         bucket_key = self.prefix[\"train_image\"] / file_name)\n",
    "        self.upload_file(file_path = (mask_dir / file_name),\n",
    "                         bucket_key = self.prefix[\"train_mask\"] / file_name)\n",
    "\n",
    "    def upload_test_pair(self, file_name, image_dir, mask_dir):\n",
    "        self.upload_file(file_path = (image_dir / file_name),\n",
    "                         bucket_key = self.prefix[\"test_image\"] / file_name)\n",
    "        self.upload_file(file_path = (mask_dir / file_name),\n",
    "                         bucket_key = self.prefix[\"test_mask\"] / file_name)\n",
    "    \n",
    "    \n",
    "    def delete_train_pair(self, file_name):\n",
    "        self.delete_file(self.prefix[\"train_image\"] / file_name)\n",
    "        self.delete_file(self.prefix[\"train_mask\"] / file_name)\n",
    "\n",
    "    def delete_test_pair(self, file_name):\n",
    "        self.delete_file(self.prefix[\"test_image\"] / file_name)\n",
    "        self.delete_file(self.prefix[\"test_mask\"] / file_name)\n",
    "\n",
    "    def list_files_in_bucket(self):\n",
    "        for file_path in self.bucket.rglob(\"*\"):\n",
    "            print(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InriaETL():\n",
    "\n",
    "    urls = {\n",
    "        \"aerialimagelabeling.7z.001\" : \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.001\",\n",
    "        \"aerialimagelabeling.7z.002\" : \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.002\",\n",
    "        \"aerialimagelabeling.7z.003\" : \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.003\",\n",
    "        \"aerialimagelabeling.7z.004\" : \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.004\",\n",
    "        \"aerialimagelabeling.7z.005\" : \"https://files.inria.fr/aerialimagelabeling/aerialimagelabeling.7z.005\"\n",
    "    }\n",
    "\n",
    "    sizes = {\n",
    "        \"aerialimagelabeling.7z.001\" : 0,\n",
    "        \"aerialimagelabeling.7z.002\" : 0,\n",
    "        \"aerialimagelabeling.7z.003\" : 0,\n",
    "        \"aerialimagelabeling.7z.004\" : 0,\n",
    "        \"aerialimagelabeling.7z.005\" : 0\n",
    "    }\n",
    "\n",
    "    locations = [\"austin\", \"chicago\", \"kitsap\", \"tyrol-w\", \"vienna\"]\n",
    "    test_files_list = [f\"{location}{num}.tif\" for location in locations for num in range(1, 7)]\n",
    "    train_files_list = [f\"{location}{num}.tif\" for location in locations for num in range(7, 37)]\n",
    "\n",
    "    def __init__(self, root: Path):\n",
    "        self.root_dir = root\n",
    "        self.download_dir = root / \"downloads\"\n",
    "\n",
    "        # Downloaded Images, Masks\n",
    "        self.d_dataset_dir = root / \"AerialImageDataset\" / \"train\"\n",
    "        self.d_image_dir = self.d_dataset_dir / \"images\"\n",
    "        self.d_mask_dir = self.d_dataset_dir / \"gt\"\n",
    "\n",
    "        # Test Images, Masks\n",
    "        self.t_image_dir = self.root_dir / \"test\" / \"images\"\n",
    "        self.t_mask_dir = self.root_dir / \"test\" / \"masks\"\n",
    "\n",
    "        #Images, Masks\n",
    "        self.image_dir = self.root_dir / \"images\"\n",
    "        self.mask_dir = self.root_dir / \"masks\"\n",
    "\n",
    "    def download(self):\n",
    "        downloader = DatasetDownloader(self.download_dir, self.urls)\n",
    "        asyncio.run(downloader.download_files())\n",
    "        #await asyncio.create_task(downloader.download_files())\n",
    "        #downloader.validate_download()\n",
    "\n",
    "    def extract(self, low_storage_mode:bool):\n",
    "        extractor = DatasetExtractor()\n",
    "\n",
    "        #Merge and Extract Dataset Zip\n",
    "        multivolume_7zip_path = self.download_dir / \"aerialimagelabeling.7z\"\n",
    "        dataset_zip_path = self.download_dir / \"NEW2-AerialImageDataset.zip\"\n",
    "\n",
    "        extractor.extract_multivolume_archive(multivolume_7zip_path, self.download_dir)\n",
    "        print(f\"Extracted Multivolume Archive to {dataset_zip_path}\")\n",
    "\n",
    "        if low_storage_mode:\n",
    "            print(\"Deleting downloaded multi-volume to save storage space\")\n",
    "            for volume in self.download_dir.glob(\"aerialimagelabeling.7z.*\"):\n",
    "                volume.unlink()\n",
    "\n",
    "        extractor.extract_zip_archive(dataset_zip_path, self.root_dir, [\"train\"])\n",
    "        print(f\"Extracted Dataset Archive to {self.root_dir}\")\n",
    "\n",
    "        if low_storage_mode:\n",
    "            print(\"Deleting extracted dataset archive to save storage space\")\n",
    "            dataset_zip_path.unlink()\n",
    "\n",
    "        #extractor.validate_extraction(files_list, dataset_dir)\n",
    "\n",
    "    def move_test_split(self):\n",
    "        self.t_image_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.t_mask_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        for file_name in self.test_files_list:\n",
    "            shutil.move((self.d_image_dir / file_name), self.t_image_dir)\n",
    "            shutil.move((self.d_mask_dir / file_name), self.t_mask_dir)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_mask(path:Path):\n",
    "        return np.expand_dims(skimage.io.imread(path), -1) # type: ignore\n",
    "\n",
    "    def crop(self, window: int):\n",
    "        self.image_dir.mkdir(exist_ok=True)\n",
    "        self.mask_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        cropper = CropUtil()\n",
    "\n",
    "        for file_name in self.train_files_list:\n",
    "\n",
    "            cropped_image_view = cropper._crop_one_scene(\n",
    "                tile_path = self.d_image_dir / file_name,\n",
    "                window = window,\n",
    "                read_scene = cropper._read_image\n",
    "            )\n",
    "\n",
    "            cropped_mask_view = cropper._crop_one_scene(\n",
    "                tile_path = self.d_mask_dir / file_name,\n",
    "                window = window,\n",
    "                read_scene = self.read_mask\n",
    "            )\n",
    "\n",
    "            for image_crop, mask_crop in zip(cropped_image_view, cropped_mask_view):\n",
    "                crop_name = str(uuid.uuid4())\n",
    "                cropper._save_as_jpeg_100(image_crop , (self.image_dir / crop_name))\n",
    "                cropper._save_as_jpeg_100(mask_crop.squeeze(), (self.mask_dir / crop_name))\n",
    "\n",
    "    def delete_downloads(self):\n",
    "        shutil.rmtree(self.d_dataset_dir.parent)\n",
    "\n",
    "    def upload_train_dataset(self, storage: ContaboStorage):\n",
    "        train_files_list = list(self.mask_dir.glob(\"*.jpg\")) \n",
    "        for file_path in tqdm(train_files_list):\n",
    "            storage.upload_train_pair(file_path.name, self.image_dir, self.mask_dir)\n",
    "\n",
    "    def upload_test_dataset(self, storage: ContaboStorage):\n",
    "        test_files_list = list(self.mask_dir.glob(\"*.tif\"))\n",
    "        for file_path in tqdm(test_files_list):\n",
    "            storage.upload_test_pair(file_path.name, self.t_image_dir, self.t_mask_dir)\n",
    "\n",
    "    def catalog_train_dataset(self):\n",
    "        \"\"\"Returns a DataFrame with training patch names and dataset name as columns\"\"\"\n",
    "\n",
    "        df = pd.DataFrame({\"name\": [x.name for x in self.image_dir.rglob(\"*.jpg\")]}) \n",
    "        df[\"dataset\"] = [self.root_dir.name]*len(df)\n",
    "        return df\n",
    "\n",
    "    def update_train_dataset(self, storage: ContaboStorage):\n",
    "\n",
    "        #Set path for catalog to be downloaded to \n",
    "        #WARINING: training catalog path must be hamed \"patches.csv\"\n",
    "        catalog_path = self.root_dir / \"metadata\" / \"patches.csv\"\n",
    "\n",
    "        #If catalog file present in local storage, remove it\n",
    "        catalog_path.unlink(missing_ok=True)\n",
    "        catalog_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        #Download catalog from bucket\n",
    "        storage.download_train_catalog(catalog_path)\n",
    "\n",
    "        #If a catalog is present in storage \n",
    "        if catalog_path.exists() and catalog_path.is_file():\n",
    "            print(\"Downloaded Catalog Found\")\n",
    "            df = pd.read_csv(catalog_path)\n",
    "\n",
    "            #Get a view of files associated with current dataset\n",
    "            dataset_df = df[df[\"dataset\"] == self.root_dir.name]\n",
    "\n",
    "            #If any files in bucket are associated with the current dataset\n",
    "            if len(dataset_df) > 0:\n",
    "               #Remove those files from storage\n",
    "                print(f\"Deleting existing patches in bucket\")\n",
    "                existing_file_names = list(dataset_df.name.apply(lambda x: x))\n",
    "                for file_name in tqdm(existing_file_names):\n",
    "                    storage.delete_train_pair(file_name)\n",
    "                \n",
    "                #Remove those files from the catalog\n",
    "                df = df.drop(dataset_df.index, axis = 0)\n",
    "\n",
    "        \n",
    "        #If catalog is not present in storage => dataset is not present in storage\n",
    "        else:  \n",
    "            #Create a new catalog placeholder\n",
    "            print(\"Downloaded Catalog Not Found, Creating New\")\n",
    "            df = pd.DataFrame({\"name\" : list(), \"dataset\" : list()})\n",
    "        \n",
    "        #Upload training dataset to bucket\n",
    "        print(\"Uploading patches to bucket\")\n",
    "        self.upload_train_dataset(storage)\n",
    "        #Add patches in image, mask dir to catalog\n",
    "        df = pd.concat([df, self.catalog_train_dataset()], axis = 0)\n",
    "        #Save catalog to local storage\n",
    "        df.to_csv(catalog_path, index = False)\n",
    "        #Upload catalog to bucket\n",
    "        storage.upload_train_catalog(catalog_path)\n",
    "        catalog_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityOSMETL():\n",
    "    urls = {\n",
    "        \"berlin.zip\": \"https://zenodo.org/record/1154821/files/berlin.zip?download=1\",\n",
    "        \"chicago.zip\": \"https://zenodo.org/record/1154821/files/chicago.zip?download=1\",\n",
    "        \"paris.zip\": \"https://zenodo.org/record/1154821/files/paris.zip?download=1\",\n",
    "        \"potsdam.zip\": \"https://zenodo.org/record/1154821/files/potsdam.zip?download=1\",\n",
    "        \"tokyo.zip\": \"https://zenodo.org/record/1154821/files/tokyo.zip?download=1\",\n",
    "        \"zurich.zip\": \"https://zenodo.org/record/1154821/files/zurich.zip?download=1\"\n",
    "    } \n",
    "\n",
    "    def __init__(self, root: Path):\n",
    "        self.root_dir = root\n",
    "        self.download_dir = root / \"downloads\"\n",
    "\n",
    "        self.d_image_dir = self.download_dir / \"images\"\n",
    "        self.d_mask_dir = self.download_dir / \"masks\"\n",
    "\n",
    "        self.image_dir = self.root_dir / \"images\"\n",
    "        self.mask_dir = self.root_dir / \"masks\"\n",
    "\n",
    "        self.t_image_dir= self.root_dir / \"test\" / \"images\"\n",
    "        self.t_mask_dir= self.root_dir / \"test\" / \"masks\"\n",
    "\n",
    "    def download(self):\n",
    "        downloader = DatasetDownloader(self.download_dir, self.urls)\n",
    "        asyncio.run(downloader.download_files())\n",
    "    \n",
    "    def _move_and_rename_files(self, file_paths) -> None:\n",
    "        for file_path in file_paths:\n",
    "            shutil.move(file_path, file_path.parents[1] / f\"{file_path.stem.split('_')[0]}.png\")\n",
    "            \n",
    "    def _extract_file_num(self, file_name: str):\n",
    "        numbers = list()\n",
    "        for char in file_name:\n",
    "            if char.isdigit():\n",
    "                numbers.append(char) \n",
    "        return int(''.join(numbers))   \n",
    "\n",
    "    def extract(self, low_storage_space:bool):\n",
    "        extractor = DatasetExtractor()\n",
    "\n",
    "        print(f\"Extracting downloaded archives\")\n",
    "        for zip_file_name in tqdm(self.urls.keys()):\n",
    "            zip_file_path = self.download_dir / zip_file_name\n",
    "            extractor.extract_zip_archive(zip_file_path, self.d_image_dir, [\"image\"])\n",
    "            extractor.extract_zip_archive(zip_file_path, self.d_mask_dir, [\"labels\"])\n",
    "\n",
    "        print(f\"Reorganizing file structure\")\n",
    "        self._move_and_rename_files(self.d_image_dir.rglob(\"*.png\"))\n",
    "        self._move_and_rename_files(self.d_mask_dir.rglob(\"*.png\"))\n",
    "\n",
    "        if low_storage_space: \n",
    "            print(f\"Deleting downloaded archives to save storage space\")\n",
    "            for zip_file_name in self.urls.keys():\n",
    "                zip_file_path = self.download_dir / zip_file_name\n",
    "                zip_file_path.unlink()\n",
    "    \n",
    "    def calculate_train_test_split(self):\n",
    "        self.files_list = [x.name for x in self.d_image_dir.rglob(\"*.png\")]\n",
    "        location_numbers = dict()\n",
    "        for file_name in self.files_list:\n",
    "            location = file_name.split('.')[0].strip('1234567890')\n",
    "            if location in location_numbers.keys():\n",
    "                location_numbers[location].append(self._extract_file_num(file_name))\n",
    "            else:\n",
    "                location_numbers[location] = list()\n",
    "\n",
    "        self.test_files_list = list()\n",
    "        for location, numbers_list in location_numbers.items():\n",
    "            for num in sorted(numbers_list)[:int(0.15*len(numbers_list))]:\n",
    "                self.test_files_list.append(f\"{location}{num}.png\")\n",
    "        \n",
    "        self.train_files_list = list(set(self.files_list).difference(self.test_files_list))\n",
    "\n",
    "    def move_test_split(self):\n",
    "        self.t_image_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.t_mask_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        for file_name in self.test_files_list:\n",
    "            shutil.move(self.d_image_dir / file_name, self.t_image_dir)\n",
    "            shutil.move(self.d_mask_dir / file_name, self.t_mask_dir)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_mask(path: Path):\n",
    "        mask = skimage.io.imread(path)[:, :, 2] # type: ignore\n",
    "        mask = np.where((mask == 0), np.uint8(255), np.uint8(0))\n",
    "        return np.expand_dims(mask, -1)\n",
    "\n",
    "    def crop(self, window: int):\n",
    "        self.image_dir.mkdir(exist_ok=True)\n",
    "        self.mask_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        cropper = CropUtil()\n",
    "\n",
    "        for file_name in self.train_files_list:\n",
    "            cropped_image_view = cropper._crop_one_scene(\n",
    "                tile_path = self.d_image_dir / file_name,\n",
    "                window = window,\n",
    "                read_scene = cropper._read_image\n",
    "            )\n",
    "\n",
    "            cropped_mask_view = cropper._crop_one_scene(\n",
    "                tile_path = self.d_mask_dir / file_name,\n",
    "                window = window,\n",
    "                read_scene = self.read_mask\n",
    "            )\n",
    "\n",
    "            for image_crop, mask_crop in zip(cropped_image_view, cropped_mask_view):\n",
    "                crop_name = str(uuid.uuid4())\n",
    "                cropper._save_as_jpeg_100(image_crop, (self.image_dir / crop_name))\n",
    "                cropper._save_as_jpeg_100(mask_crop.squeeze(), (self.mask_dir / crop_name))\n",
    "    \n",
    "    def delete_downloads(self):\n",
    "        shutil.rmtree(self.download_dir)\n",
    "\n",
    "    def upload_train_dataset(self, storage: ContaboStorage):\n",
    "        train_files_list = list(self.mask_dir.glob(\"*.jpg\")) \n",
    "        for file_path in tqdm(train_files_list):\n",
    "            storage.upload_train_pair(file_path.name, self.image_dir, self.mask_dir)\n",
    "\n",
    "    def upload_test_dataset(self, storage: ContaboStorage):\n",
    "        test_files_list = list(self.mask_dir.glob(\"*.tif\"))\n",
    "        for file_path in tqdm(test_files_list):\n",
    "            storage.upload_test_pair(file_path.name, self.t_image_dir, self.t_mask_dir)\n",
    "\n",
    "    def catalog_train_dataset(self):\n",
    "        \"\"\"Returns a DataFrame with training patch names and dataset name as columns\"\"\"\n",
    "\n",
    "        df = pd.DataFrame({\"name\": [x.name for x in self.image_dir.rglob(\"*.jpg\")]}) \n",
    "        df[\"dataset\"] = [self.root_dir.name]*len(df)\n",
    "        return df\n",
    "\n",
    "    def update_train_dataset(self, storage: ContaboStorage):\n",
    "\n",
    "        #Set path for catalog to be downloaded to \n",
    "        #WARINING: training catalog path must be hamed \"patches.csv\"\n",
    "        catalog_path = self.root_dir / \"metadata\" / \"patches.csv\"\n",
    "\n",
    "        #If catalog file present in local storage, remove it\n",
    "        catalog_path.unlink(missing_ok=True)\n",
    "        catalog_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        #Download catalog from bucket\n",
    "        storage.download_train_catalog(catalog_path)\n",
    "\n",
    "        #If a catalog is present in storage \n",
    "        if catalog_path.exists() and catalog_path.is_file():\n",
    "            print(\"Downloaded Catalog Found\")\n",
    "            df = pd.read_csv(catalog_path)\n",
    "\n",
    "            #Get a view of files associated with current dataset\n",
    "            dataset_df = df[df[\"dataset\"] == self.root_dir.name]\n",
    "\n",
    "            #If any files in bucket are associated with the current dataset\n",
    "            if len(dataset_df) > 0:\n",
    "               #Remove those files from storage\n",
    "                print(f\"Deleting existing patches in bucket\")\n",
    "                existing_file_names = list(dataset_df.name.apply(lambda x: x))\n",
    "                for file_name in tqdm(existing_file_names):\n",
    "                    storage.delete_train_pair(file_name)\n",
    "                \n",
    "                #Remove those files from the catalog\n",
    "                df = df.drop(dataset_df.index, axis = 0)\n",
    "\n",
    "        \n",
    "        #If catalog is not present in storage => dataset is not present in storage\n",
    "        else:  \n",
    "            #Create a new catalog placeholder\n",
    "            print(\"Downloaded Catalog Not Found, Creating New\")\n",
    "            df = pd.DataFrame({\"name\" : list(), \"dataset\" : list()})\n",
    "        \n",
    "        #Upload training dataset to bucket\n",
    "        print(\"Uploading patches to bucket\")\n",
    "        self.upload_train_dataset(storage)\n",
    "        #Add patches in image, mask dir to catalog\n",
    "        df = pd.concat([df, self.catalog_train_dataset()], axis = 0)\n",
    "        #Save catalog to local storage\n",
    "        df.to_csv(catalog_path, index = False)\n",
    "        #Upload catalog to bucket\n",
    "        storage.upload_train_catalog(catalog_path)\n",
    "        catalog_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISPRSETL():\n",
    "    urls = {\n",
    "        \"potsdam.zip\": \"https://seafile.projekt.uni-hannover.de/f/429be50cc79d423ab6c4/\",\n",
    "        #\"toronto.zip\": \"https://seafile.projekt.uni-hannover.de/f/fc62f9c20a8c4a34aea1/\",\n",
    "        \"vaihingen.zip\": \"https://seafile.projekt.uni-hannover.de/f/6a06a837b1f349cfa749/\",\n",
    "    }\n",
    "    password = \"CjwcipT4-P8g\"\n",
    "    cookie_name = \"sfcsrftoken\"\n",
    "\n",
    "    def __init__(self, root: Path):\n",
    "        self.root_dir = root\n",
    "        self.download_dir = root / \"downloads\"\n",
    "\n",
    "        self.d_image_dir = self.download_dir / \"images\"\n",
    "        self.d_mask_dir = self.download_dir / \"masks\"\n",
    "\n",
    "        self.image_dir = self.root_dir / \"images\"\n",
    "        self.mask_dir = self.root_dir / \"masks\"\n",
    "\n",
    "        self.t_image_dir= self.root_dir / \"test\" / \"images\"\n",
    "        self.t_mask_dir= self.root_dir / \"test\" / \"masks\"\n",
    "    \n",
    "    def download(self):\n",
    "        downloader = DatasetDownloader(self.download_dir, self.urls)\n",
    "        asyncio.run(downloader.download_files(self._download_file))\n",
    "\n",
    "    def extract(self, low_storage_mode):\n",
    "        extractor = DatasetExtractor \n",
    "        self._extract_vaihingen(extractor, low_storage_mode)\n",
    "        self._extract_potsdam(extractor, low_storage_mode)\n",
    "    \n",
    "    def calculate_train_test_split(self):\n",
    "        self.files_list = [x.name for x in self.d_image_dir.rglob(\"*.tif\")]\n",
    "        df = pd.DataFrame({\"name\": self.files_list})\n",
    "        df[\"region\"] = df.name.apply(lambda x:self._get_file_region(x))\n",
    "\n",
    "        self.test_files_list = list()\n",
    "        for region in df.region.unique():\n",
    "            region_df = df[df[\"region\"] == region]\n",
    "            self.test_files_list += sorted(list(region_df.name))[:int(0.15*len(region_df))]\n",
    "        \n",
    "        self.train_files_list = sorted(list(set(self.files_list).difference(self.test_files_list)))\n",
    "    \n",
    "    def move_test_split(self):\n",
    "        self.t_image_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.t_mask_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        for file_name in self.test_files_list:\n",
    "            shutil.move((self.d_image_dir / file_name), self.t_image_dir)\n",
    "            shutil.move((self.d_mask_dir / file_name), self.t_mask_dir)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_mask(path:Path):\n",
    "        mask = skimage.io.imread(path) # type: ignore\n",
    "        mask = np.where(mask[:, :, 1] == 0, np.uint8(255), np.uint8(0))\n",
    "        return np.expand_dims(mask, -1)\n",
    "\n",
    "    def crop(self, window: int):\n",
    "        self.image_dir.mkdir(exist_ok=True)\n",
    "        self.mask_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        cropper = CropUtil()\n",
    "\n",
    "        for file_name in self.train_files_list:\n",
    "\n",
    "            cropped_image_view = cropper._crop_one_scene(\n",
    "                tile_path = self.d_image_dir / file_name,\n",
    "                window = window,\n",
    "                read_scene = cropper._read_image\n",
    "            )\n",
    "\n",
    "            cropped_mask_view = cropper._crop_one_scene(\n",
    "                tile_path = self.d_mask_dir / file_name,\n",
    "                window = window,\n",
    "                read_scene = self.read_mask\n",
    "            )\n",
    "\n",
    "            for image_crop, mask_crop in zip(cropped_image_view, cropped_mask_view):\n",
    "                crop_name = str(uuid.uuid4())\n",
    "                cropper._save_as_jpeg_100(image_crop, (self.image_dir / crop_name))\n",
    "                cropper._save_as_jpeg_100(mask_crop.squeeze(), (self.mask_dir / crop_name))\n",
    "\n",
    "    async def _download_file(self, session, url:str, file_path:str) -> None:\n",
    "        async with session.get(url) as response:\n",
    "                cookies = session.cookie_jar.filter_cookies(url)\n",
    "                cookie_value = cookies[self.cookie_name].value\n",
    "\n",
    "        #async with session.get(url, ssl = False) as get_request:\n",
    "            #cookies = {self.cookie_name: get_request.cookies.get(self.cookie_name)}\n",
    "        payload = {\"csrfmiddlewaretoken\": cookie_value, \n",
    "                   \"password\": self.password}\n",
    "\n",
    "        async with session.post(url+\"?dl=1\", data = payload) as r:\n",
    "           async with aiofiles.open(file_path, \"wb\") as f:\n",
    "                async for chunk in r.content.iter_any():\n",
    "                    await f.write(chunk)\n",
    "\n",
    "    def _extract_vaihingen(self, extractor, low_storage_mode: bool):\n",
    "        vaihingen_zip_path = self.download_dir / \"vaihingen.zip\"\n",
    "        image_dataset_zip_path = self.download_dir / \"Vaihingen\" / \"ISPRS_semantic_labeling_Vaihingen.zip\"\n",
    "        mask_dataset_zip_path = self.download_dir / \"Vaihingen\" / \"ISPRS_semantic_labeling_Vaihingen_ground_truth_COMPLETE.zip\"\n",
    "\n",
    "        extractor.extract_zip_archive(vaihingen_zip_path, self.download_dir, [image_dataset_zip_path.name, mask_dataset_zip_path.name])\n",
    "        print(\"Extracted downloaded archives\")\n",
    "\n",
    "        if low_storage_mode:\n",
    "            #vaihingen_zip_path.unlink()\n",
    "            print(\"Deleting downloaded archive to save storage sapce\")\n",
    "\n",
    "        self.d_image_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.d_mask_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        extractor.extract_zip_archive(image_dataset_zip_path, image_dataset_zip_path.parent, [\"top\"])\n",
    "        extractor.extract_zip_archive(mask_dataset_zip_path, self.d_mask_dir)\n",
    "        print(\"Extracted downloaded dataset\")\n",
    "\n",
    "        if low_storage_mode:\n",
    "            #image_dataset_zip_path.unlink()\n",
    "            #mask_dataset_zip_path.unlink()\n",
    "            print(\"Deleted extracted archives\")\n",
    "\n",
    "        for image_path in (image_dataset_zip_path.parent / \"top\").glob(\"*.tif\"):\n",
    "            shutil.move(image_path, self.d_image_dir)\n",
    "        print(\"Moved extracted images and masks\")\n",
    "\n",
    "        if low_storage_mode:\n",
    "            #shutil.rmtree(image_dataset_zip_path.parent)\n",
    "            pass\n",
    "    \n",
    "    def _extract_potsdam(self, extractor, low_storage_mode: bool):\n",
    "        potsdam_zip_path = self.download_dir / \"potsdam.zip\"\n",
    "        images_zip_path = self.download_dir / \"Potsdam\" / \"2_Ortho_RGB.zip\"\n",
    "        masks_zip_path = self.download_dir / \"Potsdam\" / \"5_Labels_all.zip\"\n",
    "\n",
    "        extractor.extract_zip_archive(potsdam_zip_path, self.download_dir, [images_zip_path.name, masks_zip_path.name])\n",
    "        if low_storage_mode:\n",
    "            #potsdam_zip_path.unlink()\n",
    "            print(\"Deleted downloaded archive\")\n",
    "\n",
    "        images_temp_dir = images_zip_path.parent / images_zip_path.stem\n",
    "        extractor.extract_zip_archive(images_zip_path, images_zip_path.parent)\n",
    "\n",
    "        masks_temp_dir = masks_zip_path.parent / masks_zip_path.stem\n",
    "        (masks_temp_dir).mkdir(exist_ok=True)\n",
    "        extractor.extract_zip_archive(masks_zip_path, masks_temp_dir)\n",
    "\n",
    "        print(\"Extracted images and masks sub archives\")\n",
    "\n",
    "        if low_storage_mode:\n",
    "            #images_zip_path.unlink()\n",
    "            #masks_zip_path.unlink()\n",
    "            print(\"Deleted extracted sub archives\")\n",
    "\n",
    "        self.d_image_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.d_mask_dir.mkdir(exist_ok=True, parents=True)\n",
    "        #Copy Images, Masks to Correct Directories\n",
    "        for image_path in images_temp_dir.glob(\"*.tif\"):\n",
    "            shutil.move(image_path, self.d_image_dir / image_path.name.replace(\"_RGB\", \"\"))\n",
    "        for mask_path in masks_temp_dir.glob(\"*.tif\"):\n",
    "            shutil.move(mask_path, self.d_mask_dir / mask_path.name.replace(\"_label\", \"\"))\n",
    "        print(\"Moved extracted images and masks\")\n",
    "\n",
    "        if low_storage_mode:\n",
    "            #shutil.rmtree(images_zip_path.parent)\n",
    "            print(\"Cleanup directory structure\")\n",
    "\n",
    "    def _get_file_region(self, file_name):\n",
    "        if \"mosaic\" in file_name:\n",
    "            return \"vaihingen\"\n",
    "        elif \"potsdam\" in file_name:\n",
    "            return \"potsdam\"\n",
    "    \n",
    "    def delete_downloads(self):\n",
    "        shutil.rmtree(self.download_dir)\n",
    "\n",
    "    def upload_train_dataset(self, storage: ContaboStorage):\n",
    "        train_files_list = list(self.mask_dir.glob(\"*.jpg\")) \n",
    "        for file_path in tqdm(train_files_list):\n",
    "            storage.upload_train_pair(file_path.name, self.image_dir, self.mask_dir)\n",
    "\n",
    "    def upload_test_dataset(self, storage: ContaboStorage):\n",
    "        test_files_list = list(self.mask_dir.glob(\"*.tif\"))\n",
    "        for file_path in tqdm(test_files_list):\n",
    "            storage.upload_test_pair(file_path.name, self.t_image_dir, self.t_mask_dir)\n",
    "\n",
    "    def catalog_train_dataset(self):\n",
    "        \"\"\"Returns a DataFrame with training patch names and dataset name as columns\"\"\"\n",
    "\n",
    "        df = pd.DataFrame({\"name\": [x.name for x in self.image_dir.rglob(\"*.jpg\")]}) \n",
    "        df[\"dataset\"] = [self.root_dir.name]*len(df)\n",
    "        return df\n",
    "\n",
    "    def update_train_dataset(self, storage: ContaboStorage):\n",
    "\n",
    "        #Set path for catalog to be downloaded to \n",
    "        #WARINING: training catalog path must be hamed \"patches.csv\"\n",
    "        catalog_path = self.root_dir / \"metadata\" / \"patches.csv\"\n",
    "\n",
    "        #If catalog file present in local storage, remove it\n",
    "        catalog_path.unlink(missing_ok=True)\n",
    "        catalog_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        #Download catalog from bucket\n",
    "        storage.download_train_catalog(catalog_path)\n",
    "\n",
    "        #If a catalog is present in storage \n",
    "        if catalog_path.exists() and catalog_path.is_file():\n",
    "            print(\"Downloaded Catalog Found\")\n",
    "            df = pd.read_csv(catalog_path)\n",
    "\n",
    "            #Get a view of files associated with current dataset\n",
    "            dataset_df = df[df[\"dataset\"] == self.root_dir.name]\n",
    "\n",
    "            #If any files in bucket are associated with the current dataset\n",
    "            if len(dataset_df) > 0:\n",
    "               #Remove those files from storage\n",
    "                print(f\"Deleting existing patches in bucket\")\n",
    "                existing_file_names = list(dataset_df.name.apply(lambda x: x))\n",
    "                for file_name in tqdm(existing_file_names):\n",
    "                    storage.delete_train_pair(file_name)\n",
    "                \n",
    "                #Remove those files from the catalog\n",
    "                df = df.drop(dataset_df.index, axis = 0)\n",
    "\n",
    "        \n",
    "        #If catalog is not present in storage => dataset is not present in storage\n",
    "        else:  \n",
    "            #Create a new catalog placeholder\n",
    "            print(\"Downloaded Catalog Not Found, Creating New\")\n",
    "            df = pd.DataFrame({\"name\" : list(), \"dataset\" : list()})\n",
    "        \n",
    "        #Upload training dataset to bucket\n",
    "        print(\"Uploading patches to bucket\")\n",
    "        self.upload_train_dataset(storage)\n",
    "        #Add patches in image, mask dir to catalog\n",
    "        df = pd.concat([df, self.catalog_train_dataset()], axis = 0)\n",
    "        #Save catalog to local storage\n",
    "        df.to_csv(catalog_path, index = False)\n",
    "        #Upload catalog to bucket\n",
    "        storage.upload_train_catalog(catalog_path)\n",
    "        catalog_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"/media/sambhav/30AC4696AC46568E/datasets/urban-feature-extraction\")\n",
    "#DATA = Path(\"C:/Users/hp/Desktop/datasets/urban-feature-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntb = ContaboStorage(credentials_file = (Path.cwd() / \"drive\" / \"MyDrive\" / \"aws\" / \"credentials\"),\n",
    "                      bucket_name = 'building-footprints-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cntb = S3StorageEmulator(root = DATA.parent,\n",
    "                         #credentials_file = (Path.home() / \".aws\" / \"config\"), \n",
    "                         #bucket_name= 'building-footprints-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Catalog Not Found\n",
      "Downloaded Catalog Not Found, Creating New\n",
      "Uploading patches to bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [01:51<00:00, 134.71it/s]\n"
     ]
    }
   ],
   "source": [
    "inria = InriaETL(DATA / \"inria\")\n",
    "inria.download()\n",
    "inria.extract(low_storage_mode=True)\n",
    "inria.crop(512)\n",
    "inria.move_test_split()\n",
    "inria.delete_downloads()\n",
    "inria.update_train_dataset(cntb) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting downloaded archives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [06:40<00:00, 66.68s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorganizing file structure\n",
      "Downloaded Catalog Found\n",
      "Uploading patches to bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62184/62184 [08:34<00:00, 120.78it/s]\n"
     ]
    }
   ],
   "source": [
    "cityosm = CityOSMETL(DATA / \"city-osm\")\n",
    "cityosm.extract(False)\n",
    "cityosm.calculate_train_test_split()\n",
    "cityosm.move_test_split()\n",
    "cityosm.crop(512)\n",
    "#cityosm.delete_downloads()\n",
    "cityosm.update_train_dataset(cntb) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted downloaded archives\n",
      "Extracted downloaded dataset\n",
      "Moved extracted images and masks\n",
      "Extracted images and masks sub archives\n",
      "Moved extracted images and masks\n",
      "Downloaded Catalog Found\n",
      "Uploading patches to bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5402/5402 [00:25<00:00, 209.16it/s]\n"
     ]
    }
   ],
   "source": [
    "isprs = ISPRSETL(DATA / \"isprs\")\n",
    "isprs.extract(False)\n",
    "isprs.calculate_train_test_split()\n",
    "isprs.move_test_split()\n",
    "isprs.crop(512)\n",
    "#isprs.delete_downloads()\n",
    "isprs.update_train_dataset(cntb) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/hp/Desktop/datasets/building-footprints-dataset/labelled/metadata/patches.csv\", index_col=0)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
